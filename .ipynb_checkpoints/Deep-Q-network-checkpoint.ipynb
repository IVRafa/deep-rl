{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "# Unlike the blog, we will be using Keras in this example\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info about the Frozen Lake in the documentation: \n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazing blog post on DQNs which is also where I got the base code for this DQNAgent class. Must read!\n",
    "# https://keon.io/deep-q-learning/\n",
    "\n",
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_shape=(self.state_size,), activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        act_values = self.model.predict(np.array([state]))\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            target_out = self.model.predict(state)\n",
    "            target_out[0][action] = target\n",
    "            self.model.fit(state, target_out, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "#         state, action, reward, next_state, done = zip(*random.sample(self.memory,batch_size))\n",
    "        \n",
    "#         future_q = self.model.predict(np.array(next_state))\n",
    "#         target_out = self.model.predict(np.array(state))\n",
    "#         for x in range(0,batch_size):\n",
    "#             if not done[x]:\n",
    "#                 target = reward[x] + self.gamma * np.amax(future_q[x][0])\n",
    "#             else:\n",
    "#                 target = reward[x]\n",
    "#             target_out[x][action[x]] = target\n",
    "#         print(np.array(state), target_out)\n",
    "#         self.model.fit(np.array(state), target_out, epochs=1, verbose=0)\n",
    "        \n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/10 - 7 steps - 0.0 reward\n",
      "Episode 1/10 - 11 steps - 0.0 reward\n",
      "Episode 2/10 - 8 steps - 0.0 reward\n",
      "Episode 3/10 - 13 steps - 0.0 reward\n",
      "[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] [[-0.02651594  0.21572974  0.06720735  0.22053325]\n",
      " [-0.02791152  0.14811221 -0.05453014 -0.02651594]\n",
      " [ 0.14895327  0.21572974  0.06720735  0.22053325]\n",
      " [ 0.15679292  0.21572974  0.06720735  0.14895327]\n",
      " [-0.02651594  0.14811221 -0.05453014  0.09623557]\n",
      " [ 0.05067972  0.17297639 -0.10558947  0.2925347 ]\n",
      " [ 0.          0.14811221 -0.05453014  0.09623557]\n",
      " [ 0.05334707  0.30285662  0.05067972  0.05969884]\n",
      " [ 0.          0.14811221 -0.05453014  0.09623557]\n",
      " [ 0.05334707  0.05067972 -0.20677742  0.05969884]\n",
      " [ 0.05334707  0.30285662 -0.20677742 -0.02651594]\n",
      " [ 0.15679292 -0.02535598  0.06720735  0.22053325]\n",
      " [-0.02791152  0.14811221  0.14895327  0.09623557]\n",
      " [ 0.05334707  0.30285662 -0.20677742  0.05067972]\n",
      " [-0.02791152  0.14895327 -0.05453014  0.09623557]\n",
      " [-0.02791152  0.14811221 -0.05453014 -0.02651594]\n",
      " [-0.11997532  0.30285662 -0.20677742  0.05969884]\n",
      " [ 0.05334707  0.30285662 -0.20677742 -0.02651594]\n",
      " [ 0.05334707 -0.02651594 -0.20677742  0.05969884]\n",
      " [-0.02791152  0.14811221  0.14895327  0.09623557]\n",
      " [ 0.05334707  0.30285662 -0.02651594  0.05969884]\n",
      " [-0.02651594  0.14811221 -0.05453014  0.09623557]\n",
      " [-0.02791152  0.         -0.05453014  0.09623557]\n",
      " [ 0.15679292 -0.02651594  0.06720735  0.22053325]\n",
      " [-0.02791152  0.14895327 -0.05453014  0.09623557]\n",
      " [ 0.05334707  0.30285662 -0.20677742  0.05067972]\n",
      " [ 0.05067972  0.30285662 -0.20677742  0.05969884]\n",
      " [ 0.05334707  0.30285662 -0.20677742  0.05067972]\n",
      " [-0.12628981  0.17297639 -0.10558947 -0.11997532]\n",
      " [ 0.05334707  0.30285662 -0.20677742  0.05067972]\n",
      " [-0.0266905   0.         -0.21958691  0.14592001]\n",
      " [ 0.05334707  0.30285662 -0.20677742  0.05067971]]\n",
      "Episode 4/10 - 9 steps - 0.0 reward\n",
      "[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] [[ 0.15146954  0.20427658  0.0747444   0.14389607]\n",
      " [-0.02674118  0.14389607 -0.04608536  0.08865967]\n",
      " [ 0.0518067  -0.11783123 -0.19840474  0.05230264]\n",
      " [-0.02674118  0.04921636 -0.04608536  0.08865967]\n",
      " [ 0.15146954 -0.02656834  0.0747444   0.2139313 ]\n",
      " [-0.12403287  0.17008959 -0.09990827 -0.11783123]\n",
      " [-0.02540412  0.20427658  0.0747444   0.2139313 ]\n",
      " [ 0.0518067   0.29542243  0.04921636  0.05230264]\n",
      " [ 0.0518067   0.04921636 -0.19840474  0.05230264]\n",
      " [-0.02674118  0.         -0.04608536  0.08865967]\n",
      " [-0.02674118  0.14198641  0.14389607  0.08865967]\n",
      " [ 0.0518067   0.29542243 -0.02540412  0.05230264]\n",
      " [ 0.04921636  0.29542243 -0.19840474  0.05230264]\n",
      " [-0.02674118  0.14389607 -0.04608536  0.08865967]\n",
      " [ 0.14389607  0.20427658  0.0747444   0.2139313 ]\n",
      " [ 0.0518067   0.29542243 -0.19840474  0.04921636]\n",
      " [-0.02674118  0.         -0.04608536  0.08865967]\n",
      " [ 0.0518067  -0.02540412 -0.19840474  0.05230264]\n",
      " [ 0.0518067   0.29542243 -0.02540412  0.05230264]\n",
      " [-0.12403287  0.17008959 -0.09990827  0.04921636]\n",
      " [ 0.0518067   0.29542243 -0.19840474 -0.02540412]\n",
      " [ 0.0518067   0.29542243 -0.19840474  0.04921636]\n",
      " [ 0.14389607  0.20427658  0.0747444   0.2139313 ]\n",
      " [-0.02796667  0.         -0.21109462  0.13994399]\n",
      " [ 0.15146954 -0.02540412  0.0747444   0.2139313 ]\n",
      " [ 0.0518067   0.29542243 -0.19840474  0.04921636]\n",
      " [ 0.0518067   0.29542243 -0.19840474  0.04921636]\n",
      " [-0.02674118  0.14198641 -0.04608536 -0.02540412]\n",
      " [ 0.0518067   0.04921636 -0.19840474  0.05230264]\n",
      " [ 0.0518067   0.29542243 -0.02540412  0.05230264]\n",
      " [ 0.          0.14198643 -0.04608535  0.08865965]\n",
      " [-0.02674118  0.14198643 -0.04608535 -0.02540412]]\n",
      "Episode 5/10 - 3 steps - 0.0 reward\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] [[-0.02519283  0.04745003 -0.03753792  0.07962988]\n",
      " [-0.1220365  -0.06573965 -0.09405462  0.27008298]\n",
      " [ 0.14543095  0.1910885   0.08294214  0.13815941]\n",
      " [ 0.          0.13452482 -0.03753792  0.07962988]\n",
      " [-0.02519283  0.13452482 -0.03753792 -0.02393319]\n",
      " [ 0.0499474   0.04745003 -0.18885285  0.0445304 ]\n",
      " [ 0.0499474  -0.11593468 -0.18885285  0.0445304 ]\n",
      " [ 0.14543095  0.1910885   0.08294214 -0.02393319]\n",
      " [ 0.04745003  0.13452482 -0.03753792  0.07962988]\n",
      " [ 0.0499474   0.28669327 -0.18885285  0.04745003]\n",
      " [ 0.14543095  0.1910885   0.08294214 -0.02393319]\n",
      " [-0.02519283  0.13452482 -0.03753792 -0.02393319]\n",
      " [-0.02519283  0.13815941 -0.03753792  0.07962988]\n",
      " [ 0.04745003  0.28669327 -0.18885285  0.0445304 ]\n",
      " [ 0.0499474   0.28669327 -0.18885285 -0.02393319]\n",
      " [-0.02519283  0.13815941 -0.03753792  0.07962988]\n",
      " [-0.02519283  0.13452482  0.13815941  0.07962988]\n",
      " [-0.02393319  0.13452482 -0.03753792  0.07962988]\n",
      " [ 0.0499474   0.28669327  0.04745003  0.0445304 ]\n",
      " [-0.02931146  0.         -0.20159851  0.1333048 ]\n",
      " [ 0.14543095 -0.02393319  0.08294214  0.20689327]\n",
      " [-0.02519283  0.13815941 -0.03753792  0.07962988]\n",
      " [-0.02519283  0.         -0.03753792  0.07962988]\n",
      " [ 0.0499474   0.28669327 -0.18885285  0.04745003]\n",
      " [-0.02393319  0.13452482 -0.03753792  0.07962988]\n",
      " [ 0.0499474   0.28669327 -0.02393319  0.0445304 ]\n",
      " [-0.1220365   0.16652302 -0.09405462 -0.11593468]\n",
      " [-0.02393319  0.1910885   0.08294214  0.20689327]\n",
      " [-0.06919964  0.         -0.09329328  0.11314799]\n",
      " [-0.1220365   0.16652302 -0.09405462  0.04745003]\n",
      " [ 0.          0.13452484 -0.03753792  0.07962987]\n",
      " [-0.02519284  0.13452484  0.1381594   0.07962987]]\n",
      "Episode 6/10 - 15 steps - 0.0 reward\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] [[ 0.04856768  0.27788442 -0.17932108  0.0461393 ]\n",
      " [ 0.0461393   0.27788442 -0.17932108  0.03578582]\n",
      " [ 0.04856768 -0.02206997 -0.17932108  0.03578582]\n",
      " [ 0.04856768  0.27788442  0.0461393   0.03578582]\n",
      " [ 0.04856768  0.27788442 -0.02206997  0.03578582]\n",
      " [ 0.14146097  0.17883846  0.08981706  0.13438793]\n",
      " [-0.11372977  0.27788442 -0.17932108  0.03578582]\n",
      " [ 0.13438793  0.17883846  0.08981706  0.19518434]\n",
      " [-0.11971555  0.16270816 -0.08807882  0.0461393 ]\n",
      " [ 0.          0.12690754 -0.02852415  0.07007056]\n",
      " [ 0.04856768  0.27788442 -0.02206997  0.03578582]\n",
      " [-0.11971555  0.16270816 -0.08807882 -0.11372977]\n",
      " [ 0.04856768  0.27788442 -0.17932108 -0.02206997]\n",
      " [-0.02206997  0.17883846  0.08981706  0.19518434]\n",
      " [-0.02323155  0.         -0.02852415  0.07007056]\n",
      " [ 0.14146097  0.17883846  0.08981706 -0.02206997]\n",
      " [ 0.14146097 -0.02853808  0.08981706  0.19518434]\n",
      " [ 0.0461393   0.16270816 -0.08807882  0.25724697]\n",
      " [ 0.04856768  0.27788442 -0.02206997  0.03578582]\n",
      " [ 0.04856768  0.27788442 -0.11372977  0.03578582]\n",
      " [-0.02206997  0.12690754 -0.02852415  0.07007056]\n",
      " [-0.02323155  0.12690754 -0.02852415  0.0461393 ]\n",
      " [ 0.0461393   0.27788442 -0.17932108  0.03578582]\n",
      " [-0.02323155  0.13438793 -0.02852415  0.07007056]\n",
      " [-0.03004008  0.         -0.19188586  0.12523566]\n",
      " [-0.02323155  0.13438793 -0.02852415  0.07007056]\n",
      " [ 0.0461393   0.27788442 -0.17932108  0.03578582]\n",
      " [ 0.14146097  0.17883846  0.08981706  0.13438793]\n",
      " [-0.02323155  0.13438793 -0.02852415  0.07007056]\n",
      " [ 0.0461393   0.12690754 -0.02852415  0.07007056]\n",
      " [ 0.          0.12690756 -0.02852418  0.07007057]\n",
      " [-0.02323156  0.12690756  0.13438793  0.07007057]]\n",
      "Episode 7/10 - 17 steps - 0.0 reward\n",
      "[[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] [[-0.06915883 -0.06570089 -0.08495562  0.10043344]\n",
      " [ 0.04697132 -0.02010752 -0.16913171  0.02726622]\n",
      " [ 0.13763139  0.16745721  0.09687977  0.13074982]\n",
      " [-0.06915883  0.1549333   0.          0.10043344]\n",
      " [ 0.13763139 -0.02935359  0.09687977  0.18263726]\n",
      " [ 0.13763139 -0.02010752  0.09687977  0.18263726]\n",
      " [ 0.04462276  0.11984287 -0.01918785  0.06073238]\n",
      " [ 0.04697132  0.26959378 -0.02010752  0.02726622]\n",
      " [ 0.04697132  0.26959378 -0.02010752  0.02726622]\n",
      " [-0.06570089  0.1592162  -0.08200924  0.24399029]\n",
      " [ 0.04697132  0.26959378 -0.02010752  0.02726622]\n",
      " [ 0.04462276  0.26959378 -0.16913171  0.02726622]\n",
      " [-0.02116581  0.11984287 -0.01918785  0.04462276]\n",
      " [ 0.04697132  0.04462276 -0.16913171  0.02726622]\n",
      " [-0.02010752  0.16745721  0.09687977  0.18263726]\n",
      " [ 0.13763139  0.16745721  0.09687977 -0.02010752]\n",
      " [-0.11699163  0.1592162  -0.08200924 -0.11114205]\n",
      " [-0.02116581  0.11984287 -0.02010752  0.06073238]\n",
      " [ 0.04697132  0.26959378 -0.02010752  0.02726622]\n",
      " [ 0.04697132  0.26959378  0.04462276  0.02726622]\n",
      " [ 0.13763139 -0.02010752  0.09687977  0.18263726]\n",
      " [-0.02116581  0.13074982 -0.01918785  0.06073238]\n",
      " [-0.06570089  0.1549333  -0.08495562  0.10043344]\n",
      " [ 0.13074982  0.16745721  0.09687977  0.18263726]\n",
      " [ 0.04462276  0.26959378 -0.16913171  0.02726622]\n",
      " [-0.02116581  0.11984287  0.13074982  0.06073238]\n",
      " [-0.03089852  0.         -0.18158713  0.11734792]\n",
      " [-0.02116581  0.13074982 -0.01918785  0.06073238]\n",
      " [ 0.04462276  0.26959378 -0.16913171  0.02726622]\n",
      " [ 0.04697132  0.26959378 -0.11114205  0.02726622]\n",
      " [ 0.13763137  0.16745721  0.09687977 -0.02010754]\n",
      " [-0.02116583  0.         -0.01918785  0.06073238]]\n",
      "Episode 8/10 - 6 steps - 0.0 reward\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] [[-0.01912612  0.11268586  0.12724096  0.05172546]\n",
      " [-0.06556035  0.15561724 -0.07558081  0.2311775 ]\n",
      " [ 0.13393785  0.15661667  0.10397923  0.12724096]\n",
      " [ 0.04541704  0.26120827 -0.01816981  0.01918277]\n",
      " [-0.06556035  0.14953402 -0.08055526  0.09386995]\n",
      " [-0.03162003  0.         -0.17101793  0.10948011]\n",
      " [ 0.04314619  0.26120827 -0.15823911  0.01918277]\n",
      " [-0.01912612  0.12724096 -0.00956699  0.05172546]\n",
      " [ 0.04314619  0.15561724 -0.07558081  0.2311775 ]\n",
      " [ 0.13393785 -0.01816981  0.10397923  0.16922393]\n",
      " [ 0.04541704  0.26120827 -0.01816981  0.01918277]\n",
      " [ 0.04314619  0.11268586 -0.00956699  0.05172546]\n",
      " [-0.10859851  0.26120827 -0.15823911  0.01918277]\n",
      " [ 0.04541704  0.26120827 -0.10859851  0.01918277]\n",
      " [-0.01912612  0.12724096 -0.00956699  0.05172546]\n",
      " [-0.11431422  0.15561724 -0.07558081  0.04314619]\n",
      " [ 0.04541704 -0.01816981 -0.15823911  0.01918277]\n",
      " [ 0.04541704  0.26120827 -0.15823911  0.04314619]\n",
      " [-0.06901089  0.         -0.08055526  0.09386995]\n",
      " [ 0.04541704  0.04314619 -0.15823911  0.01918277]\n",
      " [ 0.04314619  0.26120827 -0.15823911  0.01918277]\n",
      " [-0.06901089 -0.06556035 -0.08055526  0.09386995]\n",
      " [ 0.04541704 -0.10859851 -0.15823911  0.01918277]\n",
      " [-0.01912612  0.11268586 -0.01816981  0.05172546]\n",
      " [-0.01912612  0.11268586 -0.00956699 -0.01816981]\n",
      " [-0.01912612  0.11268586 -0.00956699 -0.01816981]\n",
      " [-0.01912612  0.04314619 -0.00956699  0.05172546]\n",
      " [ 0.04541704  0.26120827 -0.15823911  0.04314619]\n",
      " [-0.01816981  0.11268586 -0.00956699  0.05172546]\n",
      " [-0.11431422  0.15561724  0.04314619  0.2311775 ]\n",
      " [-0.01912611  0.11268586 -0.00956698 -0.01816981]\n",
      " [ 0.04541704 -0.01816981 -0.15823913  0.01918278]]\n",
      "Episode 9/10 - 10 steps - 1.0 reward\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]] [[-0.01623252  0.10532153  0.12470277  0.04292507]\n",
      " [ 0.04225743  0.25214976 -0.1480495   0.01191   ]\n",
      " [-0.0991684   0.09184551 -0.09420998  0.2253461 ]\n",
      " [-0.0154209   0.14578816  0.11089842  0.15691343]\n",
      " [ 0.0444815   0.25214976 -0.1480495  -0.0154209 ]\n",
      " [ 0.0444815   0.25214976  0.04225743  0.01191   ]\n",
      " [ 0.04225743  0.10532153 -0.00059803  0.04292507]\n",
      " [ 0.0444815   0.25214976 -0.1480495  -0.0154209 ]\n",
      " [-0.0154209   0.10532153 -0.00059803  0.04292507]\n",
      " [ 0.0444815   0.25214976  0.04225743  0.01191   ]\n",
      " [ 0.          0.10532153 -0.00059803  0.04292507]\n",
      " [-0.01623252  0.04225743 -0.00059803  0.04292507]\n",
      " [-0.01623252  0.10532153 -0.00059803 -0.0154209 ]\n",
      " [-0.06724425  0.         -0.07638885  0.08811546]\n",
      " [-0.11075397  0.15171495 -0.06897273  0.04225743]\n",
      " [-0.0991684   0.04392932 -0.07766967  0.2253461 ]\n",
      " [-0.11075397  0.15171495  0.04225743  0.21949564]\n",
      " [ 0.0444815   0.25214976 -0.0154209   0.01191   ]\n",
      " [-0.01623252  0.10532153  0.12470277  0.04292507]\n",
      " [ 0.0444815   0.25214976 -0.1480495   0.04225743]\n",
      " [ 0.12470277  0.14578816  0.11089842  0.15691343]\n",
      " [ 0.13126607  0.14578816  0.11089842 -0.0154209 ]\n",
      " [ 0.0444815  -0.0154209  -0.1480495   0.01191   ]\n",
      " [ 0.0444815   0.25214976 -0.0154209   0.01191   ]\n",
      " [ 0.          0.10532153 -0.00059803  0.04292507]\n",
      " [-0.06724425 -0.06388204 -0.07638885  0.08811546]\n",
      " [-0.01623252  0.10532153 -0.0154209   0.04292507]\n",
      " [-0.06724425  0.14369047  0.          0.08811546]\n",
      " [ 0.0444815   0.25214976 -0.10521627  0.01191   ]\n",
      " [ 0.0444815   0.25214976 -0.1480495  -0.0154209 ]\n",
      " [ 0.0444815  -0.10521626 -0.14804952  0.01191001]\n",
      " [ 0.13126606 -0.01542089  0.11089841  0.15691341]]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "batch_size = 4\n",
    "\n",
    "# Create the agent\n",
    "agent = DQNAgent(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the env\n",
    "    pos = env.reset()\n",
    "    state = np.eye(N=1, M=16, k=pos, dtype=int)[0] # We use the numpy.eye function to create the one-hot vector\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    # We limit the amount of steps to avoid any infinite or long episodes\n",
    "    while step < 99:\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Pass the action to the environment\n",
    "        state_new, reward, done,_ = env.step(action)\n",
    "        state_new = np.eye(N=1, M=16, k=state_new, dtype=int)[0]\n",
    "\n",
    "        # Add this new experience to the agents memory\n",
    "        agent.remember(state, action, reward, state_new, done)\n",
    "   \n",
    "        # Iterate variables and check if the episode is done (in terminal state)\n",
    "        episode_reward += reward\n",
    "        step += 1       \n",
    "        state = state_new\n",
    "        if done:\n",
    "            print(\"Episode {}/{} - {} steps - {} reward\".format(episode, num_episodes, step, episode_reward))\n",
    "            break\n",
    "    \n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04340317  0.24338077 -0.13712645  0.00522141]\n",
      " [ 0.04340317  0.24338077 -0.13712645  0.00522141]\n",
      " [ 0.0439654   0.05438959  0.01695709  0.05144625]\n",
      " [-0.10781736  0.14788064 -0.06163432  0.20870192]]\n",
      "[[ 0.04340317  0.24338077 -0.01297887  0.00522141]\n",
      " [ 0.04340317  0.24338077 -0.13712645  0.04123301]\n",
      " [ 0.0439654   0.05438959  0.01695709 -0.09337959]\n",
      " [-0.10781736 -0.06240766 -0.06163432  0.20870192]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "state, action, reward, next_state, done = zip(*random.sample(agent.memory,batch_size))\n",
    "\n",
    "future_q = agent.model.predict(np.array(next_state))\n",
    "target_out = agent.model.predict(np.array(state))\n",
    "print(target_out)\n",
    "for x in range(0,batch_size):\n",
    "    if not done[x]:\n",
    "        target = reward[x] + agent.gamma * np.amax(future_q[x][0])\n",
    "    else:\n",
    "        target = reward[x]\n",
    "    \n",
    "    target_out[x][action[x]] = target\n",
    "print(target_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, False, False, True)\n",
      "[[ 0.04340317  0.24338077 -0.13712645  0.00522141]\n",
      " [-0.10781736  0.14788064 -0.06163432  0.20870192]\n",
      " [ 0.12785308  0.13542403  0.11849722  0.14539072]\n",
      " [ 0.03470756 -0.01242799  0.03000678  0.27064362]]\n"
     ]
    }
   ],
   "source": [
    "print(done)\n",
    "print(future_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01041937, -0.22935791,  0.10132787, -0.17830871]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array([[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])\n",
    "agent.model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, action, reward, next_state, done = zip(*random.sample(agent.memory,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05996166, -0.18592815, -0.26443177,  0.19104314],\n",
       "       [-0.09735326, -0.12600815, -0.2244632 ,  0.03633647],\n",
       "       [-0.09735326, -0.12600815, -0.2244632 ,  0.03633647],\n",
       "       [-0.09735326, -0.12600815, -0.2244632 ,  0.03633647],\n",
       "       [ 0.05996166, -0.18592815, -0.26443177,  0.19104314],\n",
       "       [-0.07036515, -0.03292663,  0.09776071,  0.18202789]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model.predict(np.array(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/100 - 18 steps - 0.0 reward\n",
      "Episode 1/100 - 2 steps - 0.0 reward\n",
      "Episode 2/100 - 2 steps - 0.0 reward\n",
      "Episode 3/100 - 3 steps - 0.0 reward\n",
      "Episode 4/100 - 7 steps - 0.0 reward\n",
      "Episode 5/100 - 9 steps - 0.0 reward\n",
      "Episode 6/100 - 35 steps - 0.0 reward\n",
      "Episode 7/100 - 2 steps - 0.0 reward\n",
      "Episode 8/100 - 9 steps - 0.0 reward\n",
      "Episode 9/100 - 9 steps - 0.0 reward\n",
      "Episode 10/100 - 2 steps - 0.0 reward\n",
      "Episode 11/100 - 7 steps - 0.0 reward\n",
      "Episode 12/100 - 10 steps - 0.0 reward\n",
      "Episode 13/100 - 6 steps - 0.0 reward\n",
      "Episode 14/100 - 10 steps - 0.0 reward\n",
      "Episode 15/100 - 12 steps - 0.0 reward\n",
      "Episode 16/100 - 2 steps - 0.0 reward\n",
      "Episode 17/100 - 2 steps - 0.0 reward\n",
      "Episode 18/100 - 8 steps - 0.0 reward\n",
      "Episode 19/100 - 4 steps - 0.0 reward\n",
      "Episode 20/100 - 6 steps - 0.0 reward\n",
      "Episode 21/100 - 6 steps - 0.0 reward\n",
      "Episode 22/100 - 11 steps - 0.0 reward\n",
      "Episode 23/100 - 2 steps - 0.0 reward\n",
      "Episode 24/100 - 12 steps - 0.0 reward\n",
      "Episode 25/100 - 12 steps - 0.0 reward\n",
      "Episode 26/100 - 2 steps - 0.0 reward\n",
      "Episode 27/100 - 9 steps - 0.0 reward\n",
      "Episode 28/100 - 7 steps - 0.0 reward\n",
      "Episode 29/100 - 6 steps - 0.0 reward\n",
      "Episode 30/100 - 3 steps - 0.0 reward\n",
      "Episode 31/100 - 42 steps - 0.0 reward\n",
      "Episode 32/100 - 8 steps - 0.0 reward\n",
      "Episode 33/100 - 12 steps - 0.0 reward\n",
      "Episode 34/100 - 13 steps - 0.0 reward\n",
      "Episode 35/100 - 15 steps - 0.0 reward\n",
      "Episode 36/100 - 6 steps - 0.0 reward\n",
      "Episode 37/100 - 6 steps - 0.0 reward\n",
      "Episode 38/100 - 3 steps - 0.0 reward\n",
      "Episode 39/100 - 10 steps - 0.0 reward\n",
      "Episode 40/100 - 3 steps - 0.0 reward\n",
      "Episode 41/100 - 9 steps - 0.0 reward\n",
      "Episode 42/100 - 3 steps - 0.0 reward\n",
      "Episode 43/100 - 5 steps - 0.0 reward\n",
      "Episode 44/100 - 5 steps - 0.0 reward\n",
      "Episode 45/100 - 19 steps - 0.0 reward\n",
      "Episode 46/100 - 10 steps - 0.0 reward\n",
      "Episode 47/100 - 6 steps - 0.0 reward\n",
      "Episode 48/100 - 4 steps - 0.0 reward\n",
      "Episode 49/100 - 2 steps - 0.0 reward\n",
      "Episode 50/100 - 7 steps - 0.0 reward\n",
      "Episode 51/100 - 11 steps - 0.0 reward\n",
      "Episode 52/100 - 8 steps - 0.0 reward\n",
      "Episode 53/100 - 4 steps - 0.0 reward\n",
      "Episode 54/100 - 11 steps - 0.0 reward\n",
      "Episode 55/100 - 7 steps - 0.0 reward\n",
      "Episode 56/100 - 5 steps - 0.0 reward\n",
      "Episode 57/100 - 10 steps - 0.0 reward\n",
      "Episode 58/100 - 4 steps - 0.0 reward\n",
      "Episode 59/100 - 9 steps - 0.0 reward\n",
      "Episode 60/100 - 7 steps - 0.0 reward\n",
      "Episode 61/100 - 11 steps - 0.0 reward\n",
      "Episode 62/100 - 4 steps - 0.0 reward\n",
      "Episode 63/100 - 4 steps - 0.0 reward\n",
      "Episode 64/100 - 7 steps - 0.0 reward\n",
      "Episode 65/100 - 8 steps - 0.0 reward\n",
      "Episode 66/100 - 7 steps - 0.0 reward\n",
      "Episode 67/100 - 8 steps - 0.0 reward\n",
      "Episode 68/100 - 7 steps - 0.0 reward\n",
      "Episode 69/100 - 11 steps - 0.0 reward\n",
      "Episode 70/100 - 9 steps - 0.0 reward\n",
      "Episode 71/100 - 14 steps - 0.0 reward\n",
      "Episode 72/100 - 3 steps - 0.0 reward\n",
      "Episode 73/100 - 5 steps - 0.0 reward\n",
      "Episode 74/100 - 6 steps - 0.0 reward\n",
      "Episode 75/100 - 4 steps - 0.0 reward\n",
      "Episode 76/100 - 7 steps - 0.0 reward\n",
      "Episode 77/100 - 3 steps - 0.0 reward\n",
      "Episode 78/100 - 4 steps - 0.0 reward\n",
      "Episode 79/100 - 4 steps - 0.0 reward\n",
      "Episode 80/100 - 5 steps - 0.0 reward\n",
      "Episode 81/100 - 7 steps - 0.0 reward\n",
      "Episode 82/100 - 3 steps - 0.0 reward\n",
      "Episode 83/100 - 3 steps - 0.0 reward\n",
      "Episode 84/100 - 8 steps - 0.0 reward\n",
      "Episode 85/100 - 5 steps - 0.0 reward\n",
      "Episode 86/100 - 12 steps - 0.0 reward\n",
      "Episode 87/100 - 6 steps - 0.0 reward\n",
      "Episode 88/100 - 4 steps - 0.0 reward\n",
      "Episode 89/100 - 7 steps - 0.0 reward\n",
      "Episode 90/100 - 7 steps - 0.0 reward\n",
      "Episode 91/100 - 14 steps - 0.0 reward\n",
      "Episode 92/100 - 14 steps - 0.0 reward\n",
      "Episode 93/100 - 2 steps - 0.0 reward\n",
      "Episode 94/100 - 7 steps - 0.0 reward\n",
      "Episode 95/100 - 2 steps - 0.0 reward\n",
      "Episode 96/100 - 2 steps - 0.0 reward\n",
      "Episode 97/100 - 15 steps - 0.0 reward\n",
      "Episode 98/100 - 10 steps - 0.0 reward\n",
      "Episode 99/100 - 6 steps - 0.0 reward\n"
     ]
    }
   ],
   "source": [
    "# Similar to the Q-learning table, we now test the DQN removing all randomness\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "# Create a list that stores the episode_reward and step count at each episode\n",
    "reward_list = []\n",
    "step_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the env\n",
    "    pos = env.reset()\n",
    "    state = np.eye(N=1, M=16, k=pos, dtype=int)[0] # We use the numpy.eye function to create the one-hot vector\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    # We limit the amount of steps to avoid any infinite or long episodes\n",
    "    while step < 99:\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Pass the action to the environment\n",
    "        state_new, reward, done,_ = env.step(action)\n",
    "        state_new = np.eye(N=1, M=16, k=state_new, dtype=int)[0]\n",
    "            \n",
    "        # Iterate variables and check if the episode is done (in terminal state)\n",
    "        episode_reward += reward\n",
    "        step += 1       \n",
    "        state = state_new\n",
    "        if done:\n",
    "            print(\"Episode {}/{} - {} steps - {} reward\".format(episode, num_episodes, step, episode_reward))\n",
    "            break\n",
    "    \n",
    "    # Add the episode rewards and steps to a list so we can review performance later on\n",
    "    reward_list.append(episode_reward)\n",
    "    step_list.append(step)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score over time: 0.0\n",
      "Average steps over time: 7.64\n"
     ]
    }
   ],
   "source": [
    "print (\"Average score over time: {}\".format(str(sum(reward_list)/num_episodes)))\n",
    "print (\"Average steps over time: {}\".format(str(sum(step_list)/num_episodes)))\n",
    "\n",
    "# Average score over time: 0.72\n",
    "# Average steps over time: 34.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
