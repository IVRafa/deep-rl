{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumb \"Best Pick\" Drafting Agent\n",
    "# When drafting, this agent will always pick the highest ranked player as long as it complies with the team roster requirements\n",
    "\n",
    "# TODO: Punt Agent, Okay Pick Agent\n",
    "\n",
    "class BestPickAgent:\n",
    "  def __init__(self, id, variance):\n",
    "    # Unique id to identify the agent. We start indexing at 1\n",
    "    self.id = id+1\n",
    "    self.type = \"BestPick\"\n",
    "    self.variance = variance\n",
    "    \n",
    "    # List of player names on this agents team \n",
    "    self.team = []\n",
    "    \n",
    "    # The total stats of this agents team. Calculated as the sum of the z-scores of each players stats\n",
    "    self.stats = {'FG%_z':0,'3P_z':0,'FT%_z':0,'TRB_z':0,'AST_z':0,'STL_z':0,'BLK_z':0,'TOV_z':0,'PTS_z':0, 'Overall_z':0}\n",
    "    \n",
    "    # The dumb agent is not allowed to draft more than 6 guards, 6 forwards, and 4 centers.\n",
    "    # We give each position a number representation where: PG = 1, SG = 2, SF = 3, PF = 4, C = 5\n",
    "    self.roster = {1:0, 2:0, 3:0, 4:0, 5:0}\n",
    "  \n",
    "  def draft(self, pool, names):\n",
    "    pool.loc[pool['Overall_z'].idxmax()]\n",
    "    # We set a restriction if the agent has too many players of a certain position\n",
    "    # Also restricted to only choose from available players\n",
    "    res = \"TEAM == 0\"\n",
    "    if self.roster[1] + self.roster[2] >= 6:\n",
    "      res += \" and POS != 1 and POS != 2\"\n",
    "    if self.roster[3] + self.roster[4] >= 6:\n",
    "      res += \" and POS != 3 and POS != 4\"\n",
    "    if self.roster[5] >= 4:\n",
    "      res += \" and POS != 5\"\n",
    "    \n",
    "    # Randomly choose a player from the top x available choices. Where x is the variance of the agent\n",
    "    choice = random.randrange(self.variance)\n",
    "    # Get the index of the chosen player\n",
    "    ind = pool.query(res).nlargest(choice+1,'Overall_z').iloc[[choice]].index.item()\n",
    "    \n",
    "    # Set 'team' of the chosen player to the agents id\n",
    "    pool.loc[ind, 'TEAM'] = self.id\n",
    "    \n",
    "    # Transform the player dataframe to a dictionary and remove the unnecessary keys\n",
    "    newPlayer = pool.loc[ind].to_dict()\n",
    "    self.team.append(newPlayer.pop('ID')) # Add player id to the agents team\n",
    "    newPlayer.pop('TEAM')\n",
    "    self.roster[newPlayer.pop('POS')] += 1 # Add the players position to the agents roster\n",
    "    \n",
    "    # Add the stats of the new player to the current teams stats\n",
    "    for key in self.stats.keys():\n",
    "        self.stats[key] += newPlayer[key]\n",
    "\n",
    "    return pool, ind\n",
    "  \n",
    "  def reset(self):\n",
    "    self.team = []\n",
    "    self.stats = {'FG%_z':0,'3P_z':0,'FT%_z':0,'TRB_z':0,'AST_z':0,'STL_z':0,'BLK_z':0,'TOV_z':0,'PTS_z':0, 'Overall_z':0}\n",
    "    self.roster = {1:0, 2:0, 3:0, 4:0, 5:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates the reward by simulating matchups against other agents\n",
    "def getReward(agents):\n",
    "    total = 0\n",
    "    text = \"\"\n",
    "    cats = ['FG%_z','3P_z','FT%_z','TRB_z','AST_z','STL_z','BLK_z','TOV_z','PTS_z']\n",
    "    # Get the DQN agents stats\n",
    "    for agent in agents:\n",
    "        if agent.type == \"DQN\":\n",
    "            dqnStats = agent.stats\n",
    "            rnd = len(agent.team)\n",
    "            text += \"DQN \"\n",
    "            for cat in cats:\n",
    "                text += \" {}:{:.2f}\".format(cat, dqnStats[cat])\n",
    "    \n",
    "    for agent in agents:\n",
    "        if agent.type != \"DQN\":\n",
    "            won = 0\n",
    "            lost = 0\n",
    "            tie = 0\n",
    "            text += \"\\nvs {}\".format(agent.id)\n",
    "            for cat in cats:\n",
    "                text += \" {}:{:.2f}\".format(cat, dqnStats[cat] - agent.stats[cat])\n",
    "                if dqnStats[cat] > agent.stats[cat]:\n",
    "                    won += 1\n",
    "                elif dqnStats[cat] <= agent.stats[cat]:\n",
    "                    lost += 1\n",
    "                #else:\n",
    "                    #tie += 1\n",
    "                    \n",
    "            if won >= 5:\n",
    "                # Big incentive for winning + bonus for each cat won\n",
    "                # Rewards increase in later rounds\n",
    "                #discount = rnd / 10\n",
    "                discount = 1\n",
    "                total += (10 + (3.33 * (won-5)))*discount\n",
    "            elif lost >= 5:\n",
    "                # Negative reward for every cat lost\n",
    "                #discount = rnd / 10\n",
    "                discount = 1\n",
    "                total -= (10 + (3.33 * (lost-5)))*discount\n",
    "\n",
    "            #text += (\" Won: {} Lost: {} Tie:{}\\n\".format(won, lost, tie))\n",
    "            text += (\" Won: {} Lost: {}\".format(won, lost))\n",
    "\n",
    "    return total, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a random pool of players\n",
    "def createPool(size, draftOrder):\n",
    "    # We randomly generate the z-scores of each stat\n",
    "    pool = pd.DataFrame(columns=['ID', 'POS', 'FG%_z', 'FT%_z', '3P_z', 'TRB_z', 'AST_z', 'STL_z', 'BLK_z', 'TOV_z', 'PTS_z', 'TEAM'])\n",
    "\n",
    "    '''\n",
    "    tmp = []\n",
    "    for agent in draftOrder:\n",
    "    tmp.append(agent.id)\n",
    "    tmp = np.array(tmp)\n",
    "    '''\n",
    "  \n",
    "    for i in range(size):\n",
    "        # We give each position a number representation where: PG = 1, SG = 2, SF = 3, PF = 4, C = 5\n",
    "        rand = np.random.normal(scale=2, size=9)\n",
    "        pool.loc[i] = [i, random.randint(1,5),rand[0],rand[1],rand[2],rand[3],rand[4],rand[5],rand[6],rand[7],rand[8],0]\n",
    "\n",
    "    # Create position archetypes (Centers get more rebs and blks etc.)\n",
    "\n",
    "    # Add an overall_z column which is just the sum of all the z-scores. This is treated as the players overall ranking.  \n",
    "    pool['Overall_z'] = (pool['FG%_z']+pool['3P_z']+pool['FT%_z']+pool['TRB_z']+pool['AST_z']+pool['STL_z']+pool['BLK_z']+pool['TOV_z']+pool['PTS_z'])\n",
    "\n",
    "    '''\n",
    "    for x in range(20):\n",
    "    pos = \"pos_\" + str(x)\n",
    "    if x >= len(tmp):\n",
    "      pool[pos] = 0\n",
    "    else:\n",
    "      pool[pos] = tmp[x]\n",
    "    '''\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNBAPool():\n",
    "    # Download the Avg stats of each player from the 2017-2018 season, from my gdrive\n",
    "    df = pd.read_csv('Avg_Player_Stats_18_19.csv')\n",
    "\n",
    "    # Drop unimportant columns\n",
    "    df.drop(columns=['Rk', 'Age', 'Tm', 'GS', 'MP', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'ORB', 'DRB', 'PF'], inplace=True)\n",
    "\n",
    "    # Remove players with 20 or less games played\n",
    "    df.query(\"G>20\", inplace=True)\n",
    "\n",
    "    # Calculate the z-score of each players stats\n",
    "    # We first calculate for the all of the players. Then we keep the top 250 players, then re-calculate z-score once more\n",
    "\n",
    "    cols = ['FG%', '3P', 'FT%', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PTS']\n",
    "\n",
    "    for x in range(2):\n",
    "        for col in cols:\n",
    "            col_zscore = col + '_z'\n",
    "            if col == 'TOV':\n",
    "              # More turnovers are bad, so we need to interchange some values for this stat\n",
    "              df[col_zscore] = (df[col].mean() - df[col])/df[col].std(ddof=0)\n",
    "            elif col == 'FG%':\n",
    "              # We can't just get the z-score for FG% because we also need to account for how many FG attempts this player makes\n",
    "              # Instead we get the z-scores for a players impact where:\n",
    "              # impact = difference * attempts\n",
    "              # difference = FG% - avgLeagueFG%\n",
    "\n",
    "              diff = df['FG%'] - (df['FG'].sum()/df['FGA'].sum())\n",
    "              df['FG_IMP'] = diff * df['FGA']\n",
    "              df[col_zscore] = (df['FG_IMP'] - df['FG_IMP'].mean())/df['FG_IMP'].std(ddof=0)\n",
    "\n",
    "            elif col == 'FT%':\n",
    "              # For FT%, we use the same logic as FG%\n",
    "              diff = df['FT%'] - (df['FT'].sum()/df['FTA'].sum())\n",
    "              df['FT_IMP'] = diff * df['FTA'] \n",
    "              df[col_zscore] = (df['FT_IMP'] - df['FT_IMP'].mean())/df['FT_IMP'].std(ddof=0)\n",
    "\n",
    "            else:\n",
    "              # Default z-score equation\n",
    "              # z-score = (actual.stat-mean)/std.dev\n",
    "              df[col_zscore] = (df[col] - df[col].mean())/df[col].std(ddof=0)\n",
    "\n",
    "        # Add an overall_z column which is just the sum of all the z-scores. This is treated as the players overall ranking.  \n",
    "        df['Overall_z'] = (df['FG%_z']+df['3P_z']+df['FT%_z']+df['TRB_z']+df['AST_z']+df['STL_z']+df['BLK_z']+df['TOV_z']+df['PTS_z'])\n",
    "\n",
    "        # Change the POS field to integers\n",
    "        df.loc[df['POS'] == 'PG', 'POS'] = 1\n",
    "        df.loc[df['POS'] == 'SG', 'POS'] = 2  \n",
    "        df.loc[df['POS'] == 'SF', 'POS'] = 3  \n",
    "        df.loc[df['POS'] == 'PF', 'POS'] = 4  \n",
    "        df.loc[df['POS'] == 'C', 'POS'] = 5\n",
    "        \n",
    "        \n",
    "\n",
    "        # Add a column to show which team the player belongs to. 0 means the player has not been drafted\n",
    "        df['TEAM'] = 0\n",
    "\n",
    "        # We then only keep the top 250 players, and then re-calculate the z-scores one more time\n",
    "        df = df.nlargest(280, 'Overall_z')\n",
    "\n",
    "\n",
    "        \n",
    "    df.drop(df.columns.difference(['ID', 'POS', 'FG%_z', 'FT%_z', '3P_z', 'TRB_z', 'AST_z', 'STL_z', 'BLK_z', 'TOV_z', 'PTS_z', 'Overall_z', 'TEAM']), 1, inplace=True)\n",
    "    # Change the ID column to integers\n",
    "    playerNames = df['ID'].tolist()\n",
    "    df['ID'] = range(280)\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    return df, playerNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that creates a 1d state using the pool and the agents\n",
    "# (280*13) + (12) + (12*11)\n",
    "# (pool*playerInfo) + (numTeams*cats)\n",
    "def createState(df, agents):\n",
    "    front = df.values.flatten()\n",
    "    \n",
    "    mid = []\n",
    "    for agent in agents:\n",
    "        mid.append(agent.id)\n",
    "        \n",
    "    back = [[] for _ in range(12)]\n",
    "    for agent in agents:\n",
    "        back[agent.id-1].append(agent.id)\n",
    "        for cat in agent.stats.keys():\n",
    "            back[agent.id-1].append(agent.stats[cat])\n",
    "    \n",
    "\n",
    "    return np.concatenate((front, np.array(mid).flatten(), np.array(back).flatten()))\n",
    "\n",
    "# Helper function to unpack the state so the DQN agent can use it\n",
    "def unpackState(state):\n",
    "    tmp = state[:3640]\n",
    "    tmp = tmp.reshape(280,13)\n",
    "    df = pd.DataFrame(data=tmp)\n",
    "    df.columns = ['ID', 'POS', 'FG%_z', '3P_z', 'FT%_z', 'TRB_z', 'AST_z', 'STL_z',\n",
    "       'BLK_z', 'TOV_z', 'PTS_z', 'Overall_z', 'TEAM']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fantasy Drafting Environment\n",
    "# Modeled after the Gym environments: https://www.novatec-gmbh.de/en/blog/creating-a-gym-environment/\n",
    "class DraftEnvironment:\n",
    "    def __init__(self, poolSize, DQNAgent, teamCount):\n",
    "        self.teamCount = teamCount\n",
    "        self.poolSize = poolSize\n",
    "        self.draftNum = 0\n",
    "        # Initialize the other \"dumb draft agents\" and the DQN agent placeholder\n",
    "        self.agents = []\n",
    "        for x in range(self.teamCount-1):\n",
    "            tmpAgent = MyDQNAgent(poolSize, x+1)\n",
    "            tmpAgent.load()\n",
    "            self.agents.append(tmpAgent)\n",
    "        # Initialize the player pool\n",
    "        #self.pool = pd.DataFrame(columns=['ID', 'POS', 'FG%_z', 'FT%_z', '3P_z', 'TRB_z', 'AST_z', 'STL_z', 'BLK_z', 'TOV_z', 'PTS_z', 'TEAM'])\n",
    "        #self.pool = createPool(self.poolSize, self.agents)\n",
    "        self.pool, self.playerNames = getNBAPool() #TODO: Switch back to randomized pools\n",
    "        # The state of the pool before the current action\n",
    "        self.prevPool = pd.DataFrame(columns=['ID', 'POS', 'FG%_z', 'FT%_z', '3P_z', 'TRB_z', 'AST_z', 'STL_z', 'BLK_z', 'TOV_z', 'PTS_z', 'TEAM'])\n",
    "        # Used for rendering how rewards is calculated\n",
    "        self.rewardText = \"\"\n",
    "        # Used for rendering the draft picks\n",
    "        self.draftHistory = []\n",
    "\n",
    "    def step(self, action):\n",
    "        # Action is the index of the player being drafted\n",
    "\n",
    "        # Mark the DQN's player as drafted\n",
    "        self.prevPool = self.pool\n",
    "        self.pool.loc[action, 'TEAM'] = 1\n",
    "        self.draftNum += 1\n",
    "        self.draftHistory.append((1, action))\n",
    "\n",
    "        # Dumb agents after the DQN agent drafts\n",
    "        while True: \n",
    "            if self.draftNum >= self.teamCount:\n",
    "                # Draft is done when everyone has 13 players\n",
    "                if len(self.agents[self.draftNum-1].team) >= 13:\n",
    "                    #return np.array([[self.pool.values, self.prevPool.values]]), getReward(self.agents), 1, 0\n",
    "                    reward, self.rewardText = getReward(self.agents)\n",
    "                    return createState(self.pool, self.agents), reward, 1, 0\n",
    "\n",
    "                # Simulate snake draft  \n",
    "                self.agents.reverse()\n",
    "                self.draftNum = 0\n",
    "                # Calculate the reward\n",
    "                reward, self.rewardText = getReward(self.agents)\n",
    "\n",
    "            if self.agents[self.draftNum].type != \"DQN\" and len(self.agents[self.draftNum].team) < 13:\n",
    "                self.prevPool = self.pool\n",
    "                self.pool, ind = self.agents[self.draftNum].draft(self.pool, self.playerNames)\n",
    "                self.draftHistory.append((self.agents[self.draftNum].id, ind))\n",
    "                self.draftNum += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Returns next_state, reward, done, info\n",
    "        return createState(self.pool, self.agents), reward, 0, 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Shuffle draft order\n",
    "        random.shuffle(self.agents)\n",
    "        self.draftNum = 0\n",
    "        self.draftHistory = []\n",
    "        # Randomize the player pool\n",
    "        #self.pool = createPool(self.poolSize, self.agents)\n",
    "        self.pool['TEAM'] = 0\n",
    "        # Clear all teams and return a clean \"state\"\n",
    "        for agent in self.agents:\n",
    "            agent.reset()   \n",
    "        # Draft players for all dumb agents in earlier draft position than DQN agent\n",
    "        while True:\n",
    "            ind = 0\n",
    "            if self.agents[self.draftNum].type != \"DQN\":\n",
    "                self.pool, ind = self.agents[self.draftNum].draft(self.pool, self.playerNames)\n",
    "                self.draftHistory.append((self.agents[self.draftNum].id, ind))\n",
    "                self.draftNum += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return createState(self.pool, self.agents)\n",
    "\n",
    "    def render(self):\n",
    "        print(self.rewardText)\n",
    "        \n",
    "        '''\n",
    "        cnt = 0\n",
    "        for x in range(13):         \n",
    "            print(\"Round {}\".format(x+1))\n",
    "            for y in range(self.teamCount):\n",
    "                print(\"Agent {} drafts {}\".format(self.draftHistory[cnt+y][0], self.playerNames[self.draftHistory[cnt+y][1]]))\n",
    "            print(\"\")\n",
    "            cnt += self.teamCount\n",
    "        '''\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "# Taken from https://keon.io/deep-q-learning/ with some adjustments\n",
    "class MyDQNAgent:\n",
    "    def __init__(self, pool_size, id):\n",
    "        self.type = \"DQN\"\n",
    "        self.id = id\n",
    "        self.team = []   \n",
    "        self.stats = {'FG%_z':0,'3P_z':0,'FT%_z':0,'TRB_z':0,'AST_z':0,'STL_z':0,'BLK_z':0,'TOV_z':0,'PTS_z':0, 'Overall_z':0}   \n",
    "        self.roster = {1:0, 2:0, 3:0, 4:0, 5:0}\n",
    "        # State size = current and prev frame * # of players in the pool * # of variables per player\n",
    "        #self.state_size = 2 * pool_size * 13\n",
    "        # State size = # of players in the pool * # of variables per player\n",
    "        self.state_size = pool_size * 13 + 12 + 12 * 11\n",
    "        self.action_size = pool_size\n",
    "        self.memory = collections.deque(maxlen=8000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 0.01  # exploration rate\n",
    "        self.epsilon_min = 0.01 ###\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Is 256 nodes too much?\n",
    "        model.add(Dense(128, input_shape=(self.state_size,), activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "      \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Agent is only able to choose from undrafted players\n",
    "        # TODO: The agent is also not allowed to draft too many players of a certain position (How to implement this in replay()?)\n",
    "        \n",
    "        res = \"TEAM == 0\"\n",
    "        '''\n",
    "        if self.roster[1] + self.roster[2] >= 6:\n",
    "          res += \" and POS != 1 and POS != 2\"\n",
    "        if self.roster[3] + self.roster[4] >= 6:\n",
    "          res += \" and POS != 3 and POS != 4\"\n",
    "        if self.roster[5] >= 4:\n",
    "          res += \" and POS != 5\"\n",
    "        '''\n",
    "        unp = unpackState(state)\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Get the index of the randomly chosen undrafted player\n",
    "            # Instead of getting a random player, we choose randomly from the top 15 players\n",
    "            # TODO: Try fully random instead of top 15\n",
    "            choice = random.randrange(15)\n",
    "            ind = unp.query(res).nlargest(choice+1,'Overall_z').iloc[[choice]].index.item()\n",
    "\n",
    "        else:\n",
    "            df = unp.copy()\n",
    "            # act_values is the predicted reward if an action is taken\n",
    "            act_values = self.model.predict(np.array([state]))\n",
    "            df[\"act_values\"] = act_values[0]         \n",
    "          \n",
    "            # Get the index of the undrafted player with the largest 'act_value'\n",
    "            ind = df.query(res).nlargest(1, 'act_values').index.item()\n",
    "\n",
    "        # Transform the player dataframe to a dictionary and remove the unnecessary keys\n",
    "        newPlayer = unp.loc[ind].to_dict()\n",
    "        self.team.append(newPlayer.pop('ID')) # Add player id to the agents team\n",
    "        newPlayer.pop('TEAM')\n",
    "        self.roster[newPlayer.pop('POS')] += 1 # Add the players position to the agents roster\n",
    "\n",
    "        # Add the stats of the new player to the current teams stats\n",
    "        for key in self.stats.keys():\n",
    "            self.stats[key] += newPlayer[key]  \n",
    "          \n",
    "        return ind\n",
    "      \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                res = \"TEAM == 0\"\n",
    "                next_df = unpackState(next_state)\n",
    "                act_values = self.model.predict(np.array([next_state]))\n",
    "                next_df[\"act_values\"] = act_values[0]\n",
    "                target = reward + (self.gamma * next_df.query(res)['act_values'].max())              \n",
    "\n",
    "            target_f = self.model.predict(np.array([state]))\n",
    "            #print(\"{} -> {} + {}\".format(target_f[0][action], reward,self.gamma * next_df.query(res)['act_values'].max()))\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def reset(self):\n",
    "        self.type = \"DQN\"\n",
    "        self.id = 1\n",
    "        self.team = []   \n",
    "        self.stats = {'FG%_z':0,'3P_z':0,'FT%_z':0,'TRB_z':0,'AST_z':0,'STL_z':0,'BLK_z':0,'TOV_z':0,'PTS_z':0, 'Overall_z':0}   \n",
    "        self.roster = {1:0, 2:0, 3:0, 4:0, 5:0}\n",
    "    \n",
    "    def save(self):\n",
    "        # Save the model weights\n",
    "        self.model.save_weights('FantasyDraft-DQN.h5')\n",
    "    \n",
    "    def load(self):\n",
    "        self.model.load_weights('FantasyDraft-DQN.h5')\n",
    "        print(\"Succesfully loaded weights!\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 03:19:20.618001 139758875342592 deprecation_wrapper.py:119] From /home/rafaelvcantero/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0828 03:19:20.634458 139758875342592 deprecation_wrapper.py:119] From /home/rafaelvcantero/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0828 03:19:20.637503 139758875342592 deprecation_wrapper.py:119] From /home/rafaelvcantero/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0828 03:19:20.692251 139758875342592 deprecation_wrapper.py:119] From /home/rafaelvcantero/.local/lib/python3.5/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0828 03:19:20.718560 139758875342592 deprecation_wrapper.py:119] From /home/rafaelvcantero/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0828 03:19:20.720539 139758875342592 deprecation_wrapper.py:119] From /home/rafaelvcantero/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded weights!\n",
      "Succesfully loaded weights!\n",
      "Succesfully loaded weights!\n",
      "Succesfully loaded weights!\n",
      "Succesfully loaded weights!\n",
      "Succesfully loaded weights!\n",
      "Succesfully loaded weights!\n",
      "Succesfully loaded weights!\n",
      "Succesfully loaded weights!\n",
      "Succesfully loaded weights!\n",
      "Succesfully loaded weights!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_21_input to have shape (3784,) but got array with shape (3663,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f6ce171e4a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# DQN Agent decides action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Move to the next state given the DQN Agents actiono\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-4a489464d7c9>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# act_values is the predicted reward if an action is taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"act_values\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_21_input to have shape (3784,) but got array with shape (3663,)"
     ]
    }
   ],
   "source": [
    "# TODO: Multiagent Training\n",
    "# TODO: Reward function should be much bigger for the final round\n",
    "\n",
    "# Looks like the agent is learning to punt 3P, FT%, and TO\n",
    "# TODO: Try running the agent with the 2017-2018 stats.\n",
    "\n",
    "# Training the DQN agent\n",
    "\n",
    "# Initialize the environment\n",
    "poolSize = 280\n",
    "numRounds = 13\n",
    "numTeams = 12\n",
    "env = DraftEnvironment(poolSize, _, numTeams)\n",
    "numEpisodes = 1\n",
    "done = 0\n",
    "\n",
    "metricsEvery = 1\n",
    "renderEvery = 1\n",
    "roundScore = 0\n",
    "roundOverall = 0\n",
    "avgScore = []\n",
    "avgOverall = []\n",
    "\n",
    "fo = open(\"Logs-FantasyDraft-DQN.txt\", \"w\")\n",
    "\n",
    "timeStart = time.time()\n",
    "# Iterate the draft\n",
    "for e in range(numEpisodes):\n",
    "  \n",
    "    # Reset environment\n",
    "    state = env.reset()\n",
    "  \n",
    "    # Until the drafting process is done\n",
    "    for r in range(numRounds):\n",
    "    \n",
    "        for agent in env.agents:\n",
    "            # DQN Agent decides action\n",
    "            action = agent.act(state)\n",
    "            # Move to the next state given the DQN Agents actiono\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Save the experience to memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            # Update current state\n",
    "            state = next_state\n",
    "\n",
    "            #roundScore += reward\n",
    "    \n",
    "    # Print the agents score\n",
    "    avgScore.append(roundScore)\n",
    "    roundScore = 0\n",
    "    avgOverall.append(agent.stats['Overall_z'])\n",
    "        \n",
    "    if e % metricsEvery == 0:\n",
    "        text = \"episode: {}/{}, score: {}, overall_Z: {}, time: {}\".format(e, numEpisodes, sum(avgScore)/len(avgScore), sum(avgOverall)/len(avgOverall), datetime.timedelta(seconds=time.time() - timeStart))\n",
    "        print(text)\n",
    "        fo.write(text+'\\n')\n",
    "        avgScore = []\n",
    "        avgOverall = []\n",
    "        agent.save()\n",
    "\n",
    "    if e % renderEvery == 0 and e > 0:\n",
    "        env.render()\n",
    "            \n",
    "    if len(agent.memory) > 128*10:\n",
    "        # Train the agent using its experiences\n",
    "        agent.replay(128)\n",
    "        \n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in env.agents:\n",
    "    print(agent.id)\n",
    "    print(len(agent.team))\n",
    "    print(agent.stats)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.pool.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poolSize = 280\n",
    "numRounds = 13\n",
    "numTeams = 12\n",
    "agent = MyDQNAgent(poolSize, 1)\n",
    "agent.load()\n",
    "env = DraftEnvironment(poolSize, agent, numTeams)\n",
    "# Reset environment\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent decides action\n",
    "action = agent.act(state)\n",
    "# Move to the next state given the DQN Agents action\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "agent.remember(state, action, reward, next_state, done)\n",
    "# Update current state\n",
    "state = next_state\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['act_values'].max()\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "print(env.agents[x].team)\n",
    "print(env.agents[x].stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+=1\n",
    "print(env.agents[x].team)\n",
    "print(env.agents[x].stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script FantasyDraft-DQN.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
