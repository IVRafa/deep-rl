{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "#https://keon.io/deep-q-learning/\n",
    "\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info about the Frozen Lake in the documentation: \n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/100000\n",
      "Episode 10000/100000\n",
      "Episode 20000/100000\n",
      "Episode 30000/100000\n",
      "Episode 40000/100000\n",
      "Episode 50000/100000\n",
      "Episode 60000/100000\n",
      "Episode 70000/100000\n",
      "Episode 80000/100000\n",
      "Episode 90000/100000\n"
     ]
    }
   ],
   "source": [
    "# Initialize a table representing each state-action pair\n",
    "# For a 4x4 board, with 4 directions, we have a Q table with dimensions 16x4\n",
    "# We can use the gym env attributes observation_space, and action_space for this\n",
    "qTable = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "\n",
    "# Learning parameters used in the blog post\n",
    "lr = .8\n",
    "discount = .95\n",
    "num_episodes = 100000\n",
    "# Epsilon is a value that is multiplied to the noise. \n",
    "# We start with a big epsilon for exploration and gradually decrease it overtime\n",
    "epsilon = 1\n",
    "epsilon_min = 0.001\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "# Create a list that stores the episode_reward and step count at each episode\n",
    "reward_list = []\n",
    "step_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the env\n",
    "    state = env.reset() # State is the current board position (from 0 to env.observation_space.n) \n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    # We limit the amount of steps to avoid any infinite or long episodes\n",
    "    while step < 99:\n",
    "        # Generate gaussian noise to add to the action\n",
    "        noise = np.random.randn(1,env.action_space.n)*epsilon\n",
    "        action = np.argmax(qTable[state,:] + noise)\n",
    "        \n",
    "        # Pass the action to the environment\n",
    "        state_new, reward, done,_ = env.step(action)\n",
    "        \n",
    "        # Lets add a negative reward and see if we can get a better score\n",
    "#         if done and reward == 0:\n",
    "#             reward = -0.05\n",
    "#             reward = -0.01\n",
    "        \n",
    "        # Update the Q table\n",
    "        qTable[state,action] = qTable[state,action] + lr*(reward + discount*np.amax(qTable[state_new,:]) - qTable[state,action]) \n",
    "        # Decay epsilon\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "            \n",
    "        # Iterate variables and check if the episode is done (in terminal state)\n",
    "        episode_reward += reward\n",
    "        step += 1       \n",
    "        state = state_new\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Add the episode rewards and steps to a list so we can review performance later on\n",
    "    reward_list.append(episode_reward)\n",
    "    step_list.append(step)\n",
    "    \n",
    "    if episode % (num_episodes/10) == 0:\n",
    "        print(\"Episode {}/{}\".format(episode, num_episodes))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score over time: 0.71105\n",
      "Average steps over time: 41.22192\n"
     ]
    }
   ],
   "source": [
    "print (\"Average score over time: {}\".format(str(sum(reward_list)/num_episodes)))\n",
    "print (\"Average steps over time: {}\".format(str(sum(step_list)/num_episodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.81041622e-01, 4.30624456e-03, 4.10854747e-03, 3.51862573e-03],\n",
       "       [5.64516558e-04, 8.34065051e-04, 4.17687711e-04, 1.61594004e-01],\n",
       "       [2.76030073e-03, 1.88322089e-01, 2.51539734e-03, 2.83352594e-03],\n",
       "       [1.92825442e-03, 2.01972636e-03, 3.38970586e-05, 8.80076790e-02],\n",
       "       [2.74918513e-01, 1.63705118e-05, 1.14413177e-03, 4.32030413e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [7.34711082e-05, 5.38488410e-04, 1.33203177e-01, 6.57135265e-08],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.11809731e-03, 2.27595966e-04, 6.79438019e-04, 4.48511616e-01],\n",
       "       [6.49834644e-04, 6.18881370e-01, 2.08159573e-03, 1.72082868e-04],\n",
       "       [8.63355540e-01, 3.40536824e-04, 9.17668819e-04, 6.83657207e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.70921249e-02, 2.12926464e-02, 4.19775170e-01, 2.80166400e-02],\n",
       "       [0.00000000e+00, 9.90357517e-01, 5.88583710e-02, 6.46152291e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual Q table values\n",
    "qTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets test each reward system on 100 episodes\n",
    "# Remember to remove any noise/randomness when doing actual testing\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "# Create a list that stores the episode_reward at each episode\n",
    "reward_list = []\n",
    "step_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the env\n",
    "    state = env.reset() # State is the current board position (from 0 to env.observation_space.n) \n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    # We limit the amount of steps to avoid any infinite or long episodes\n",
    "    while step < 99:\n",
    "       # env.render()\n",
    "        # Get the best action without any noise. We don't want any randomness now that we are testing.\n",
    "        action = np.argmax(qTable[state,:])\n",
    "        \n",
    "        # Pass the action to the environment\n",
    "        state_new, reward, done,_ = env.step(action)\n",
    "            \n",
    "        # Iterate variables and check if the episode is done (in terminal state)\n",
    "        episode_reward += reward\n",
    "        step += 1       \n",
    "        state = state_new\n",
    "        if done == True:\n",
    "            #env.render()\n",
    "            break\n",
    "    \n",
    "    reward_list.append(episode_reward)\n",
    "    step_list.append(step)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score over time: 0.74\n",
      "Average steps over time: 38.04\n"
     ]
    }
   ],
   "source": [
    "# No negative reward\n",
    "print (\"Average score over time: {}\".format(str(sum(reward_list)/num_episodes)))\n",
    "print (\"Average steps over time: {}\".format(str(sum(step_list)/num_episodes)))\n",
    "\n",
    "# Average score: 0.74\n",
    "# Average steps: 38.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score over time: 0.0\n",
      "Average steps over time: 99.0\n"
     ]
    }
   ],
   "source": [
    "# Negative reward of -0.05\n",
    "print (\"Average score over time: {}\".format(str(sum(reward_list)/num_episodes)))\n",
    "print (\"Average steps over time: {}\".format(str(sum(step_list)/num_episodes)))\n",
    "\n",
    "# Average score: 0.0\n",
    "# Average steps: 99.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score over time: 0.54\n",
      "Average steps over time: 62.68\n"
     ]
    }
   ],
   "source": [
    "# Negative reward of -0.01\n",
    "print (\"Average score over time: {}\".format(str(sum(reward_list)/num_episodes)))\n",
    "print (\"Average steps over time: {}\".format(str(sum(step_list)/num_episodes)))\n",
    "\n",
    "# Average score: 0.54\n",
    "# Average steps: 62.68"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
