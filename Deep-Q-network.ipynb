{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "# Unlike the blog, we will be using Keras in this example\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info about the Frozen Lake in the documentation: \n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazing blog post on DQNs which is also where I got the base code for this DQNAgent class. Must read!\n",
    "# https://keon.io/deep-q-learning/\n",
    "\n",
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_shape=(self.state_size,), activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        act_values = self.model.predict(np.array([state]))\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            target_out = self.model.predict(state)\n",
    "            target_out[0][action] = target\n",
    "            self.model.fit(state, target_out, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "#         state, action, reward, next_state, done = zip(*random.sample(self.memory,batch_size))\n",
    "        \n",
    "#         future_q = self.model.predict(np.array(next_state))\n",
    "#         target_out = self.model.predict(np.array(state))\n",
    "#         for x in range(0,batch_size):\n",
    "#             if not done[x]:\n",
    "#                 target = reward[x] + self.gamma * np.amax(future_q[x][0])\n",
    "#             else:\n",
    "#                 target = reward[x]\n",
    "#             target_out[x][action[x]] = target\n",
    "#         print(np.array(state), target_out)\n",
    "#         self.model.fit(np.array(state), target_out, epochs=1, verbose=0)\n",
    "        \n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rafaelvcantero/.conda/envs/tf2/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Episode 0/10 - 6 steps - 0.0 reward\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (16,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5dc6efa39175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-13ddca6860a5>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mtarget_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mtarget_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (16,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "batch_size = 4\n",
    "\n",
    "# Create the agent\n",
    "agent = DQNAgent(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the env\n",
    "    pos = env.reset()\n",
    "    state = np.eye(N=1, M=16, k=pos, dtype=int)[0] # We use the numpy.eye function to create the one-hot vector\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    # We limit the amount of steps to avoid any infinite or long episodes\n",
    "    while step < 99:\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Pass the action to the environment\n",
    "        state_new, reward, done,_ = env.step(action)\n",
    "        state_new = np.eye(N=1, M=16, k=state_new, dtype=int)[0]\n",
    "\n",
    "        # Add this new experience to the agents memory\n",
    "        agent.remember(state, action, reward, state_new, done)\n",
    "   \n",
    "        # Iterate variables and check if the episode is done (in terminal state)\n",
    "        episode_reward += reward\n",
    "        step += 1       \n",
    "        state = state_new\n",
    "        if done:\n",
    "            print(\"Episode {}/{} - {} steps - {} reward\".format(episode, num_episodes, step, episode_reward))\n",
    "            break\n",
    "    \n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1ec5141f4c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfuture_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtarget_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "state, action, reward, next_state, done = zip(*random.sample(agent.memory,batch_size))\n",
    "\n",
    "future_q = agent.model.predict(np.array(next_state))\n",
    "target_out = agent.model.predict(np.array(state))\n",
    "print(target_out)\n",
    "for x in range(0,batch_size):\n",
    "    if not done[x]:\n",
    "        target = reward[x] + agent.gamma * np.amax(future_q[x][0])\n",
    "    else:\n",
    "        target = reward[x]\n",
    "    \n",
    "    target_out[x][action[x]] = target\n",
    "print(target_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, False, False, True)\n",
      "[[ 0.04340317  0.24338077 -0.13712645  0.00522141]\n",
      " [-0.10781736  0.14788064 -0.06163432  0.20870192]\n",
      " [ 0.12785308  0.13542403  0.11849722  0.14539072]\n",
      " [ 0.03470756 -0.01242799  0.03000678  0.27064362]]\n"
     ]
    }
   ],
   "source": [
    "print(done)\n",
    "print(future_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01041937, -0.22935791,  0.10132787, -0.17830871]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array([[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])\n",
    "agent.model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, action, reward, next_state, done = zip(*random.sample(agent.memory,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05996166, -0.18592815, -0.26443177,  0.19104314],\n",
       "       [-0.09735326, -0.12600815, -0.2244632 ,  0.03633647],\n",
       "       [-0.09735326, -0.12600815, -0.2244632 ,  0.03633647],\n",
       "       [-0.09735326, -0.12600815, -0.2244632 ,  0.03633647],\n",
       "       [ 0.05996166, -0.18592815, -0.26443177,  0.19104314],\n",
       "       [-0.07036515, -0.03292663,  0.09776071,  0.18202789]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model.predict(np.array(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/100 - 18 steps - 0.0 reward\n",
      "Episode 1/100 - 2 steps - 0.0 reward\n",
      "Episode 2/100 - 2 steps - 0.0 reward\n",
      "Episode 3/100 - 3 steps - 0.0 reward\n",
      "Episode 4/100 - 7 steps - 0.0 reward\n",
      "Episode 5/100 - 9 steps - 0.0 reward\n",
      "Episode 6/100 - 35 steps - 0.0 reward\n",
      "Episode 7/100 - 2 steps - 0.0 reward\n",
      "Episode 8/100 - 9 steps - 0.0 reward\n",
      "Episode 9/100 - 9 steps - 0.0 reward\n",
      "Episode 10/100 - 2 steps - 0.0 reward\n",
      "Episode 11/100 - 7 steps - 0.0 reward\n",
      "Episode 12/100 - 10 steps - 0.0 reward\n",
      "Episode 13/100 - 6 steps - 0.0 reward\n",
      "Episode 14/100 - 10 steps - 0.0 reward\n",
      "Episode 15/100 - 12 steps - 0.0 reward\n",
      "Episode 16/100 - 2 steps - 0.0 reward\n",
      "Episode 17/100 - 2 steps - 0.0 reward\n",
      "Episode 18/100 - 8 steps - 0.0 reward\n",
      "Episode 19/100 - 4 steps - 0.0 reward\n",
      "Episode 20/100 - 6 steps - 0.0 reward\n",
      "Episode 21/100 - 6 steps - 0.0 reward\n",
      "Episode 22/100 - 11 steps - 0.0 reward\n",
      "Episode 23/100 - 2 steps - 0.0 reward\n",
      "Episode 24/100 - 12 steps - 0.0 reward\n",
      "Episode 25/100 - 12 steps - 0.0 reward\n",
      "Episode 26/100 - 2 steps - 0.0 reward\n",
      "Episode 27/100 - 9 steps - 0.0 reward\n",
      "Episode 28/100 - 7 steps - 0.0 reward\n",
      "Episode 29/100 - 6 steps - 0.0 reward\n",
      "Episode 30/100 - 3 steps - 0.0 reward\n",
      "Episode 31/100 - 42 steps - 0.0 reward\n",
      "Episode 32/100 - 8 steps - 0.0 reward\n",
      "Episode 33/100 - 12 steps - 0.0 reward\n",
      "Episode 34/100 - 13 steps - 0.0 reward\n",
      "Episode 35/100 - 15 steps - 0.0 reward\n",
      "Episode 36/100 - 6 steps - 0.0 reward\n",
      "Episode 37/100 - 6 steps - 0.0 reward\n",
      "Episode 38/100 - 3 steps - 0.0 reward\n",
      "Episode 39/100 - 10 steps - 0.0 reward\n",
      "Episode 40/100 - 3 steps - 0.0 reward\n",
      "Episode 41/100 - 9 steps - 0.0 reward\n",
      "Episode 42/100 - 3 steps - 0.0 reward\n",
      "Episode 43/100 - 5 steps - 0.0 reward\n",
      "Episode 44/100 - 5 steps - 0.0 reward\n",
      "Episode 45/100 - 19 steps - 0.0 reward\n",
      "Episode 46/100 - 10 steps - 0.0 reward\n",
      "Episode 47/100 - 6 steps - 0.0 reward\n",
      "Episode 48/100 - 4 steps - 0.0 reward\n",
      "Episode 49/100 - 2 steps - 0.0 reward\n",
      "Episode 50/100 - 7 steps - 0.0 reward\n",
      "Episode 51/100 - 11 steps - 0.0 reward\n",
      "Episode 52/100 - 8 steps - 0.0 reward\n",
      "Episode 53/100 - 4 steps - 0.0 reward\n",
      "Episode 54/100 - 11 steps - 0.0 reward\n",
      "Episode 55/100 - 7 steps - 0.0 reward\n",
      "Episode 56/100 - 5 steps - 0.0 reward\n",
      "Episode 57/100 - 10 steps - 0.0 reward\n",
      "Episode 58/100 - 4 steps - 0.0 reward\n",
      "Episode 59/100 - 9 steps - 0.0 reward\n",
      "Episode 60/100 - 7 steps - 0.0 reward\n",
      "Episode 61/100 - 11 steps - 0.0 reward\n",
      "Episode 62/100 - 4 steps - 0.0 reward\n",
      "Episode 63/100 - 4 steps - 0.0 reward\n",
      "Episode 64/100 - 7 steps - 0.0 reward\n",
      "Episode 65/100 - 8 steps - 0.0 reward\n",
      "Episode 66/100 - 7 steps - 0.0 reward\n",
      "Episode 67/100 - 8 steps - 0.0 reward\n",
      "Episode 68/100 - 7 steps - 0.0 reward\n",
      "Episode 69/100 - 11 steps - 0.0 reward\n",
      "Episode 70/100 - 9 steps - 0.0 reward\n",
      "Episode 71/100 - 14 steps - 0.0 reward\n",
      "Episode 72/100 - 3 steps - 0.0 reward\n",
      "Episode 73/100 - 5 steps - 0.0 reward\n",
      "Episode 74/100 - 6 steps - 0.0 reward\n",
      "Episode 75/100 - 4 steps - 0.0 reward\n",
      "Episode 76/100 - 7 steps - 0.0 reward\n",
      "Episode 77/100 - 3 steps - 0.0 reward\n",
      "Episode 78/100 - 4 steps - 0.0 reward\n",
      "Episode 79/100 - 4 steps - 0.0 reward\n",
      "Episode 80/100 - 5 steps - 0.0 reward\n",
      "Episode 81/100 - 7 steps - 0.0 reward\n",
      "Episode 82/100 - 3 steps - 0.0 reward\n",
      "Episode 83/100 - 3 steps - 0.0 reward\n",
      "Episode 84/100 - 8 steps - 0.0 reward\n",
      "Episode 85/100 - 5 steps - 0.0 reward\n",
      "Episode 86/100 - 12 steps - 0.0 reward\n",
      "Episode 87/100 - 6 steps - 0.0 reward\n",
      "Episode 88/100 - 4 steps - 0.0 reward\n",
      "Episode 89/100 - 7 steps - 0.0 reward\n",
      "Episode 90/100 - 7 steps - 0.0 reward\n",
      "Episode 91/100 - 14 steps - 0.0 reward\n",
      "Episode 92/100 - 14 steps - 0.0 reward\n",
      "Episode 93/100 - 2 steps - 0.0 reward\n",
      "Episode 94/100 - 7 steps - 0.0 reward\n",
      "Episode 95/100 - 2 steps - 0.0 reward\n",
      "Episode 96/100 - 2 steps - 0.0 reward\n",
      "Episode 97/100 - 15 steps - 0.0 reward\n",
      "Episode 98/100 - 10 steps - 0.0 reward\n",
      "Episode 99/100 - 6 steps - 0.0 reward\n"
     ]
    }
   ],
   "source": [
    "# Similar to the Q-learning table, we now test the DQN removing all randomness\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "# Create a list that stores the episode_reward and step count at each episode\n",
    "reward_list = []\n",
    "step_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the env\n",
    "    pos = env.reset()\n",
    "    state = np.eye(N=1, M=16, k=pos, dtype=int)[0] # We use the numpy.eye function to create the one-hot vector\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    # We limit the amount of steps to avoid any infinite or long episodes\n",
    "    while step < 99:\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Pass the action to the environment\n",
    "        state_new, reward, done,_ = env.step(action)\n",
    "        state_new = np.eye(N=1, M=16, k=state_new, dtype=int)[0]\n",
    "            \n",
    "        # Iterate variables and check if the episode is done (in terminal state)\n",
    "        episode_reward += reward\n",
    "        step += 1       \n",
    "        state = state_new\n",
    "        if done:\n",
    "            print(\"Episode {}/{} - {} steps - {} reward\".format(episode, num_episodes, step, episode_reward))\n",
    "            break\n",
    "    \n",
    "    # Add the episode rewards and steps to a list so we can review performance later on\n",
    "    reward_list.append(episode_reward)\n",
    "    step_list.append(step)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score over time: 0.0\n",
      "Average steps over time: 7.64\n"
     ]
    }
   ],
   "source": [
    "print (\"Average score over time: {}\".format(str(sum(reward_list)/num_episodes)))\n",
    "print (\"Average steps over time: {}\".format(str(sum(step_list)/num_episodes)))\n",
    "\n",
    "# Average score over time: 0.72\n",
    "# Average steps over time: 34.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
