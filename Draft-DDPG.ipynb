{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:81: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:97: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [-1501.467]\n",
      "1: [-1284.0538]\n",
      "2: [-1653.043]\n",
      "3: [-1319.1731]\n",
      "4: [-1406.5262]\n",
      "5: [-2285.9695]\n",
      "6: [-1343.9875]\n",
      "7: [-1129.6478]\n",
      "8: [-1839.04]\n",
      "9: [-2142.1401]\n",
      "10: [-1464.2701]\n",
      "11: [-1551.0315]\n",
      "12: [-1644.6771]\n",
      "13: [-1318.1273]\n",
      "14: [-636.8655]\n",
      "15: [-1312.78]\n",
      "16: [-757.46387]\n",
      "17: [-1410.1135]\n",
      "18: [-1211.9519]\n",
      "19: [-1408.6826]\n",
      "20: [-1095.4803]\n",
      "21: [-2090.9104]\n",
      "22: [-1524.5411]\n",
      "23: [-2182.7988]\n",
      "24: [-1633.4805]\n",
      "25: [-1923.3347]\n",
      "26: [-1525.2859]\n",
      "27: [-757.0795]\n",
      "28: [-877.10315]\n",
      "29: [-1774.0839]\n",
      "30: [-2138.8264]\n",
      "31: [-1674.1084]\n",
      "32: [-2199.084]\n",
      "33: [-1571.8267]\n",
      "34: [-1010.07074]\n",
      "35: [-1817.2031]\n",
      "36: [-756.4791]\n",
      "37: [-1838.3187]\n",
      "38: [-1981.8088]\n",
      "39: [-1510.0609]\n",
      "40: [-1836.9452]\n",
      "41: [-1354.0303]\n",
      "42: [-1467.7723]\n",
      "43: [-1815.6306]\n",
      "44: [-2215.4265]\n",
      "45: [-2070.4062]\n",
      "46: [-1813.5115]\n",
      "47: [-1625.2777]\n",
      "48: [-1104.4839]\n",
      "49: [-2380.2676]\n",
      "50: [-1105.7938]\n",
      "51: [-2270.3613]\n",
      "52: [-1942.0732]\n",
      "53: [-984.6796]\n",
      "54: [-1312.8069]\n",
      "55: [-2085.38]\n",
      "56: [-2097.6104]\n",
      "57: [-1464.2548]\n",
      "58: [-1987.2421]\n",
      "59: [-1912.1405]\n",
      "60: [-1600.9287]\n",
      "61: [-1208.3405]\n",
      "62: [-1376.2657]\n",
      "63: [-1797.0764]\n",
      "64: [-765.4958]\n",
      "65: [-1432.3733]\n",
      "66: [-1168.9746]\n",
      "67: [-750.5868]\n",
      "68: [-703.09143]\n",
      "69: [-1307.589]\n",
      "70: [-902.9717]\n",
      "71: [-1868.463]\n",
      "72: [-2177.3782]\n",
      "73: [-2030.3298]\n",
      "74: [-864.2175]\n",
      "75: [-1193.5768]\n",
      "76: [-973.4603]\n",
      "77: [-1868.0142]\n",
      "78: [-1894.2865]\n",
      "79: [-1595.9027]\n",
      "80: [-757.03204]\n",
      "81: [-1260.9188]\n",
      "82: [-874.86285]\n",
      "83: [-2048.051]\n",
      "84: [-1301.8759]\n",
      "85: [-1061.6038]\n",
      "86: [-1520.3903]\n",
      "87: [-2185.2183]\n",
      "88: [-1536.3512]\n",
      "89: [-2027.8044]\n",
      "90: [-1866.0797]\n",
      "91: [-1948.063]\n",
      "92: [-1665.8602]\n",
      "93: [-1224.4823]\n",
      "94: [-1104.3153]\n",
      "95: [-804.0446]\n",
      "96: [-1740.2471]\n",
      "97: [-1410.0062]\n",
      "98: [-2312.3384]\n",
      "99: [-753.89307]\n",
      "100: [-1693.9728]\n",
      "101: [-1982.4918]\n",
      "102: [-2440.7488]\n",
      "103: [-2226.0764]\n",
      "104: [-1106.4277]\n",
      "105: [-1284.2042]\n",
      "106: [-2028.0281]\n",
      "107: [-1985.1426]\n",
      "108: [-1152.3536]\n",
      "109: [-1887.2268]\n",
      "110: [-2299.9114]\n",
      "111: [-965.0982]\n",
      "112: [-2064.2444]\n",
      "113: [-1104.824]\n",
      "114: [-1964.0404]\n",
      "115: [-871.4574]\n",
      "116: [-2283.5552]\n",
      "117: [-1409.4718]\n",
      "118: [-1974.9559]\n",
      "119: [-1613.1663]\n",
      "120: [-1229.3073]\n",
      "121: [-1676.036]\n",
      "122: [-753.25586]\n",
      "123: [-2281.4233]\n",
      "124: [-1413.5807]\n",
      "125: [-1673.2715]\n",
      "126: [-1679.8552]\n",
      "127: [-1694.8392]\n",
      "128: [-875.428]\n",
      "129: [-1402.6179]\n",
      "130: [-1685.2181]\n",
      "131: [-1592.2054]\n",
      "132: [-2178.9531]\n",
      "133: [-993.0265]\n",
      "134: [-769.41754]\n",
      "135: [-1369.4043]\n",
      "136: [-1312.1549]\n",
      "137: [-995.01917]\n",
      "138: [-991.2819]\n",
      "139: [-2019.7972]\n",
      "140: [-2239.8108]\n",
      "141: [-2360.5618]\n",
      "142: [-998.12524]\n",
      "143: [-1502.7771]\n",
      "144: [-1409.2253]\n",
      "145: [-1221.0469]\n",
      "146: [-1106.033]\n",
      "147: [-2187.573]\n",
      "148: [-1824.8079]\n",
      "149: [-2069.443]\n",
      "150: [-1397.5969]\n",
      "151: [-1368.175]\n",
      "152: [-1496.8237]\n",
      "153: [-993.6868]\n",
      "154: [-2268.5842]\n",
      "155: [-1820.4487]\n",
      "156: [-1755.3549]\n",
      "157: [-1493.6295]\n",
      "158: [-1105.9911]\n",
      "159: [-1212.9756]\n",
      "160: [-1831.8427]\n",
      "161: [-1497.194]\n",
      "162: [-1105.267]\n",
      "163: [-1613.371]\n",
      "164: [-1104.5684]\n",
      "165: [-2066.9124]\n",
      "166: [-1991.9686]\n",
      "167: [-1954.8055]\n",
      "168: [-750.89557]\n",
      "169: [-2082.1516]\n",
      "170: [-1104.2065]\n",
      "171: [-1887.1232]\n",
      "172: [-1753.8187]\n",
      "173: [-1873.8365]\n",
      "174: [-1545.144]\n",
      "175: [-1483.0493]\n",
      "176: [-2017.8961]\n",
      "177: [-1924.3821]\n",
      "178: [-1221.0283]\n",
      "179: [-1574.0833]\n",
      "180: [-756.7423]\n",
      "181: [-1698.9501]\n",
      "182: [-754.10614]\n",
      "183: [-1835.3478]\n",
      "184: [-1307.9033]\n",
      "185: [-1067.3734]\n",
      "186: [-1693.1881]\n",
      "187: [-1000.06964]\n",
      "188: [-2020.5474]\n",
      "189: [-2229.22]\n",
      "190: [-2203.9788]\n",
      "191: [-2203.5325]\n",
      "192: [-1866.8506]\n",
      "193: [-1587.6837]\n",
      "194: [-1638.2932]\n",
      "195: [-1776.1256]\n",
      "196: [-2161.014]\n",
      "197: [-1527.352]\n",
      "198: [-2233.9287]\n",
      "199: [-1829.9327]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0372ec681cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0372ec681cd2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0372ec681cd2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0372ec681cd2>\u001b[0m in \u001b[0;36m_train_critic\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mtarget_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_actor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 future_reward = self.target_critic_model.predict(\n\u001b[1;32m    130\u001b[0m                     [new_state, target_action])[0][0]\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "solving pendulum using actor-critic model\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers.merge import Add, Multiply\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "# determines how to assign values to each state, i.e. takes the state\n",
    "# and action (two-input model) and determines the corresponding value\n",
    "class ActorCritic:\n",
    "    def __init__(self, env, sess):\n",
    "        self.env  = env\n",
    "        self.sess = sess\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = .995\n",
    "        self.gamma = .95\n",
    "        self.tau   = .125\n",
    "\n",
    "        # ===================================================================== #\n",
    "        #                               Actor Model                             #\n",
    "        # Chain rule: find the gradient of chaging the actor network params in  #\n",
    "        # getting closest to the final value network predictions, i.e. de/dA    #\n",
    "        # Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\n",
    "        # ===================================================================== #\n",
    "\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.actor_state_input, self.actor_model = self.create_actor_model()\n",
    "        _, self.target_actor_model = self.create_actor_model()\n",
    "\n",
    "        self.actor_critic_grad = tf.placeholder(tf.float32, \n",
    "            [None, self.env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
    "\n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_model.output, \n",
    "            actor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "\n",
    "        # ===================================================================== #\n",
    "        #                              Critic Model                             #\n",
    "        # ===================================================================== #\t\t\n",
    "\n",
    "        self.critic_state_input, self.critic_action_input, \\\n",
    "            self.critic_model = self.create_critic_model()\n",
    "        _, _, self.target_critic_model = self.create_critic_model()\n",
    "\n",
    "        self.critic_grads = tf.gradients(self.critic_model.output, \n",
    "            self.critic_action_input) # where we calcaulte de/dC for feeding above\n",
    "\n",
    "        # Initialize for later gradient calculations\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                              Model Definitions                            #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def create_actor_model(self):\n",
    "        state_input = Input(shape=self.env.observation_space.shape)\n",
    "        h1 = Dense(24, activation='relu')(state_input)\n",
    "        h2 = Dense(48, activation='relu')(h1)\n",
    "        h3 = Dense(24, activation='relu')(h2)\n",
    "        output = Dense(self.env.action_space.shape[0], activation='relu')(h3)\n",
    "\n",
    "        model = Model(input=state_input, output=output)\n",
    "        adam  = Adam(lr=0.001)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return state_input, model\n",
    "\n",
    "    def create_critic_model(self):\n",
    "        state_input = Input(shape=self.env.observation_space.shape)\n",
    "        state_h1 = Dense(24, activation='relu')(state_input)\n",
    "        state_h2 = Dense(48)(state_h1)\n",
    "\n",
    "        action_input = Input(shape=self.env.action_space.shape)\n",
    "        action_h1    = Dense(48)(action_input)\n",
    "\n",
    "        merged    = Add()([state_h2, action_h1])\n",
    "        merged_h1 = Dense(24, activation='relu')(merged)\n",
    "        output = Dense(1, activation='relu')(merged_h1)\n",
    "        model  = Model(input=[state_input,action_input], output=output)\n",
    "\n",
    "        adam  = Adam(lr=0.001)\n",
    "        model.compile(loss=\"mse\", optimizer=adam)\n",
    "        return state_input, action_input, model\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                               Model Training                              #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def remember(self, cur_state, action, reward, new_state, done):\n",
    "        self.memory.append([cur_state, action, reward, new_state, done])\n",
    "\n",
    "    def _train_actor(self, samples):\n",
    "        for sample in samples:\n",
    "            cur_state, action, reward, new_state, _ = sample\n",
    "            predicted_action = self.actor_model.predict(cur_state)\n",
    "            grads = self.sess.run(self.critic_grads, feed_dict={\n",
    "                self.critic_state_input:  cur_state,\n",
    "                self.critic_action_input: predicted_action\n",
    "            })[0]\n",
    "\n",
    "            self.sess.run(self.optimize, feed_dict={\n",
    "                self.actor_state_input: cur_state,\n",
    "                self.actor_critic_grad: grads\n",
    "            })\n",
    "\n",
    "    def _train_critic(self, samples):\n",
    "        for sample in samples:\n",
    "            cur_state, action, reward, new_state, done = sample\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model.predict(new_state)\n",
    "                future_reward = self.target_critic_model.predict(\n",
    "                    [new_state, target_action])[0][0]\n",
    "                reward += self.gamma * future_reward\n",
    "            self.critic_model.fit([cur_state, action], reward, verbose=0)\n",
    "\n",
    "    def train(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        rewards = []\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        self._train_critic(samples)\n",
    "        self._train_actor(samples)\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                         Target Model Updating                             #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def _update_actor_target(self):\n",
    "        actor_model_weights  = self.actor_model.get_weights()\n",
    "        actor_target_weights = self.target_critic_model.get_weights()\n",
    "\n",
    "        for i in range(len(actor_target_weights)):\n",
    "            actor_target_weights[i] = actor_model_weights[i]\n",
    "        self.target_critic_model.set_weights(actor_target_weights)\n",
    "\n",
    "    def _update_critic_target(self):\n",
    "        critic_model_weights  = self.critic_model.get_weights()\n",
    "        critic_target_weights = self.critic_target_model.get_weights()\n",
    "\n",
    "        for i in range(len(critic_target_weights)):\n",
    "            critic_target_weights[i] = critic_model_weights[i]\n",
    "        self.critic_target_model.set_weights(critic_target_weights)\t\t\n",
    "\n",
    "    def update_target(self):\n",
    "        self._update_actor_target()\n",
    "        self._update_critic_target()\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                              Model Predictions                            #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def act(self, cur_state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return self.actor_model.predict(cur_state)\n",
    "\n",
    "def main():\n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)\n",
    "    env = gym.make(\"Pendulum-v0\")\n",
    "    actor_critic = ActorCritic(env, sess)\n",
    "\n",
    "    num_trials = 10000\n",
    "    trial_len  = 250\n",
    "    \n",
    "    for x in range(num_trials):\n",
    "        score = 0\n",
    "        steps = 0\n",
    "        cur_state = env.reset()\n",
    "        action = env.action_space.sample()\n",
    "        while steps <= trial_len:\n",
    "            #env.render()\n",
    "            cur_state = cur_state.reshape((1, env.observation_space.shape[0]))\n",
    "            action = actor_critic.act(cur_state)\n",
    "            action = action.reshape((1, env.action_space.shape[0]))\n",
    "\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "            actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "            actor_critic.train()\n",
    "\n",
    "            cur_state = new_state\n",
    "            \n",
    "            score += reward\n",
    "            steps += 1\n",
    "        print(\"{}: {}\".format(x, score))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
