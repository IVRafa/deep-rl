{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumb \"Best Pick\" Drafting Agent\n",
    "# When drafting, this agent will always pick the highest ranked player as long as it complies with the team roster requirements\n",
    "\n",
    "# TODO: Punt Agent, Okay Pick Agent\n",
    "\n",
    "class BestPickAgent:\n",
    "  def __init__(self, id, variance):\n",
    "    # Unique id to identify the agent. We start indexing at 1\n",
    "    self.id = id+1\n",
    "    self.type = \"BestPick\"\n",
    "    self.variance = variance\n",
    "    \n",
    "    # List of player names on this agents team \n",
    "    self.team = []\n",
    "    \n",
    "    # The total stats of this agents team. Calculated as the sum of the z-scores of each players stats\n",
    "    self.stats = {'FG%_z':0,'3P_z':0,'FT%_z':0,'TRB_z':0,'AST_z':0,'STL_z':0,'BLK_z':0,'TOV_z':0,'PTS_z':0, 'Overall_z':0}\n",
    "    \n",
    "    # The dumb agent is not allowed to draft more than 6 guards, 6 forwards, and 4 centers.\n",
    "    # We give each position a number representation where: PG = 1, SG = 2, SF = 3, PF = 4, C = 5\n",
    "    self.roster = {1:0, 2:0, 3:0, 4:0, 5:0}\n",
    "  \n",
    "  def draft(self, pool, names):\n",
    "    pool.loc[pool['Overall_z'].idxmax()]\n",
    "    # We set a restriction if the agent has too many players of a certain position\n",
    "    # Also restricted to only choose from available players\n",
    "    res = \"TEAM == 0\"\n",
    "    if self.roster[1] + self.roster[2] >= 6:\n",
    "      res += \" and POS != 1 and POS != 2\"\n",
    "    if self.roster[3] + self.roster[4] >= 6:\n",
    "      res += \" and POS != 3 and POS != 4\"\n",
    "    if self.roster[5] >= 4:\n",
    "      res += \" and POS != 5\"\n",
    "    \n",
    "    # Randomly choose a player from the top x available choices. Where x is the variance of the agent\n",
    "    choice = random.randrange(self.variance)\n",
    "    # Get the index of the chosen player\n",
    "    ind = pool.query(res).nlargest(choice+1,'Overall_z').iloc[[choice]].index.item()\n",
    "    \n",
    "    # Set 'team' of the chosen player to the agents id\n",
    "    pool.loc[ind, 'TEAM'] = self.id\n",
    "    \n",
    "    # Transform the player dataframe to a dictionary and remove the unnecessary keys\n",
    "    newPlayer = pool.loc[ind].to_dict()\n",
    "    self.team.append(newPlayer.pop('ID')) # Add player id to the agents team\n",
    "    newPlayer.pop('TEAM')\n",
    "    self.roster[newPlayer.pop('POS')] += 1 # Add the players position to the agents roster\n",
    "    \n",
    "    # Add the stats of the new player to the current teams stats\n",
    "    for key in self.stats.keys():\n",
    "        self.stats[key] += newPlayer[key]\n",
    "\n",
    "    return pool, ind\n",
    "  \n",
    "  def reset(self):\n",
    "    self.team = []\n",
    "    self.stats = {'FG%_z':0,'3P_z':0,'FT%_z':0,'TRB_z':0,'AST_z':0,'STL_z':0,'BLK_z':0,'TOV_z':0,'PTS_z':0, 'Overall_z':0}\n",
    "    self.roster = {1:0, 2:0, 3:0, 4:0, 5:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that calculates the reward by simulating matchups against other agents\n",
    "def getReward(agents):\n",
    "    total = 0\n",
    "    text = \"\"\n",
    "    cats = ['FG%_z','3P_z','FT%_z','TRB_z','AST_z','STL_z','BLK_z','TOV_z','PTS_z']\n",
    "    # Get the DQN agents stats\n",
    "    for agent in agents:\n",
    "        if agent.type == \"DQN\":\n",
    "            dqnStats = agent.stats\n",
    "            rnd = len(agent.team)\n",
    "            text += \"DQN \"\n",
    "            for cat in cats:\n",
    "                text += \" {}:{:.2f}\".format(cat, dqnStats[cat])\n",
    "    \n",
    "    for agent in agents:\n",
    "        if agent.type != \"DQN\":\n",
    "            won = 0\n",
    "            lost = 0\n",
    "            tie = 0\n",
    "            text += \"\\nvs {}\".format(agent.id)\n",
    "            for cat in cats:\n",
    "                text += \" {}:{:.2f}\".format(cat, dqnStats[cat] - agent.stats[cat])\n",
    "                if dqnStats[cat] > agent.stats[cat]:\n",
    "                    won += 1\n",
    "                elif dqnStats[cat] <= agent.stats[cat]:\n",
    "                    lost += 1\n",
    "                #else:\n",
    "                    #tie += 1\n",
    "                    \n",
    "            if won >= 5:\n",
    "                # Big incentive for winning + bonus for each cat won\n",
    "                # Rewards increase in later rounds\n",
    "                #discount = rnd / 10\n",
    "                discount = 1\n",
    "                total += (10 + (3.33 * (won-5)))*discount\n",
    "            elif lost >= 5:\n",
    "                # Negative reward for every cat lost\n",
    "                #discount = rnd / 10\n",
    "                discount = 1\n",
    "                total -= (10 + (3.33 * (lost-5)))*discount\n",
    "\n",
    "            #text += (\" Won: {} Lost: {} Tie:{}\\n\".format(won, lost, tie))\n",
    "            text += (\" Won: {} Lost: {}\".format(won, lost))\n",
    "\n",
    "    return total, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a random pool of players\n",
    "def createPool(size, draftOrder):\n",
    "    # We randomly generate the z-scores of each stat\n",
    "    pool = pd.DataFrame(columns=['ID', 'POS', 'FG%_z', 'FT%_z', '3P_z', 'TRB_z', 'AST_z', 'STL_z', 'BLK_z', 'TOV_z', 'PTS_z', 'TEAM'])\n",
    "\n",
    "    '''\n",
    "    tmp = []\n",
    "    for agent in draftOrder:\n",
    "    tmp.append(agent.id)\n",
    "    tmp = np.array(tmp)\n",
    "    '''\n",
    "  \n",
    "    for i in range(size):\n",
    "        # We give each position a number representation where: PG = 1, SG = 2, SF = 3, PF = 4, C = 5\n",
    "        rand = np.random.normal(scale=2, size=9)\n",
    "        pool.loc[i] = [i, random.randint(1,5),rand[0],rand[1],rand[2],rand[3],rand[4],rand[5],rand[6],rand[7],rand[8],0]\n",
    "\n",
    "    # Create position archetypes (Centers get more rebs and blks etc.)\n",
    "\n",
    "    # Add an overall_z column which is just the sum of all the z-scores. This is treated as the players overall ranking.  \n",
    "    pool['Overall_z'] = (pool['FG%_z']+pool['3P_z']+pool['FT%_z']+pool['TRB_z']+pool['AST_z']+pool['STL_z']+pool['BLK_z']+pool['TOV_z']+pool['PTS_z'])\n",
    "\n",
    "    '''\n",
    "    for x in range(20):\n",
    "    pos = \"pos_\" + str(x)\n",
    "    if x >= len(tmp):\n",
    "      pool[pos] = 0\n",
    "    else:\n",
    "      pool[pos] = tmp[x]\n",
    "    '''\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNBAPool():\n",
    "    # Download the Avg stats of each player from the 2017-2018 season, from my gdrive\n",
    "    df = pd.read_csv('Avg_Player_Stats_18_19.csv')\n",
    "\n",
    "    # Drop unimportant columns\n",
    "    df.drop(columns=['Rk', 'Age', 'Tm', 'GS', 'MP', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'ORB', 'DRB', 'PF'], inplace=True)\n",
    "\n",
    "    # Remove players with 20 or less games played\n",
    "    df.query(\"G>20\", inplace=True)\n",
    "\n",
    "    # Calculate the z-score of each players stats\n",
    "    # We first calculate for the all of the players. Then we keep the top 250 players, then re-calculate z-score once more\n",
    "\n",
    "    cols = ['FG%', '3P', 'FT%', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PTS']\n",
    "\n",
    "    for x in range(2):\n",
    "        for col in cols:\n",
    "            col_zscore = col + '_z'\n",
    "            if col == 'TOV':\n",
    "              # More turnovers are bad, so we need to interchange some values for this stat\n",
    "              df[col_zscore] = (df[col].mean() - df[col])/df[col].std(ddof=0)\n",
    "            elif col == 'FG%':\n",
    "              # We can't just get the z-score for FG% because we also need to account for how many FG attempts this player makes\n",
    "              # Instead we get the z-scores for a players impact where:\n",
    "              # impact = difference * attempts\n",
    "              # difference = FG% - avgLeagueFG%\n",
    "\n",
    "              diff = df['FG%'] - (df['FG'].sum()/df['FGA'].sum())\n",
    "              df['FG_IMP'] = diff * df['FGA']\n",
    "              df[col_zscore] = (df['FG_IMP'] - df['FG_IMP'].mean())/df['FG_IMP'].std(ddof=0)\n",
    "\n",
    "            elif col == 'FT%':\n",
    "              # For FT%, we use the same logic as FG%\n",
    "              diff = df['FT%'] - (df['FT'].sum()/df['FTA'].sum())\n",
    "              df['FT_IMP'] = diff * df['FTA'] \n",
    "              df[col_zscore] = (df['FT_IMP'] - df['FT_IMP'].mean())/df['FT_IMP'].std(ddof=0)\n",
    "\n",
    "            else:\n",
    "              # Default z-score equation\n",
    "              # z-score = (actual.stat-mean)/std.dev\n",
    "              df[col_zscore] = (df[col] - df[col].mean())/df[col].std(ddof=0)\n",
    "\n",
    "        # Add an overall_z column which is just the sum of all the z-scores. This is treated as the players overall ranking.  \n",
    "        df['Overall_z'] = (df['FG%_z']+df['3P_z']+df['FT%_z']+df['TRB_z']+df['AST_z']+df['STL_z']+df['BLK_z']+df['TOV_z']+df['PTS_z'])\n",
    "\n",
    "        # Change the POS field to integers\n",
    "        df.loc[df['POS'] == 'PG', 'POS'] = 1\n",
    "        df.loc[df['POS'] == 'SG', 'POS'] = 2  \n",
    "        df.loc[df['POS'] == 'SF', 'POS'] = 3  \n",
    "        df.loc[df['POS'] == 'PF', 'POS'] = 4  \n",
    "        df.loc[df['POS'] == 'C', 'POS'] = 5\n",
    "        \n",
    "        \n",
    "\n",
    "        # Add a column to show which team the player belongs to. 0 means the player has not been drafted\n",
    "        df['TEAM'] = 0\n",
    "\n",
    "        # We then only keep the top 250 players, and then re-calculate the z-scores one more time\n",
    "        df = df.nlargest(280, 'Overall_z')\n",
    "\n",
    "\n",
    "        \n",
    "    df.drop(df.columns.difference(['ID', 'POS', 'FG%_z', 'FT%_z', '3P_z', 'TRB_z', 'AST_z', 'STL_z', 'BLK_z', 'TOV_z', 'PTS_z', 'Overall_z', 'TEAM']), 1, inplace=True)\n",
    "    # Change the ID column to integers\n",
    "    playerNames = df['ID'].tolist()\n",
    "    df['ID'] = range(280)\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    return df, playerNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that creates a 1d state using the pool and the agents\n",
    "# (280*13) + (12) + (12*11)\n",
    "# (pool*playerInfo) + (numTeams*cats)\n",
    "def createState(df, agents):\n",
    "    front = df.values.flatten()\n",
    "    \n",
    "    mid = []\n",
    "    for agent in agents:\n",
    "        mid.append(agent.id)\n",
    "        \n",
    "    back = [[] for _ in range(12)]\n",
    "    for agent in agents:\n",
    "        back[agent.id-1].append(agent.id)\n",
    "        for cat in agent.stats.keys():\n",
    "            back[agent.id-1].append(agent.stats[cat])\n",
    "    \n",
    "\n",
    "    return np.concatenate((front, np.array(mid).flatten(), np.array(back).flatten()))\n",
    "\n",
    "# Helper function to unpack the state so the DQN agent can use it\n",
    "def unpackState(state):\n",
    "    tmp = state[:3640]\n",
    "    tmp = tmp.reshape(280,13)\n",
    "    df = pd.DataFrame(data=tmp)\n",
    "    df.columns = ['ID', 'POS', 'FG%_z', '3P_z', 'FT%_z', 'TRB_z', 'AST_z', 'STL_z',\n",
    "       'BLK_z', 'TOV_z', 'PTS_z', 'Overall_z', 'TEAM']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fantasy Drafting Environment\n",
    "# Modeled after the Gym environments: https://www.novatec-gmbh.de/en/blog/creating-a-gym-environment/\n",
    "class DraftEnvironment:\n",
    "    def __init__(self, poolSize, DQNAgent, teamCount):\n",
    "        self.teamCount = teamCount\n",
    "        self.poolSize = poolSize\n",
    "        self.draftNum = 0\n",
    "        # Initialize the other \"dumb draft agents\" and the DQN agent placeholder\n",
    "        self.agents = [DQNAgent]\n",
    "        for x in range(self.teamCount-1):\n",
    "            self.agents.append(BestPickAgent(x+1, x+1))\n",
    "        # Initialize the player pool\n",
    "        #self.pool = pd.DataFrame(columns=['ID', 'POS', 'FG%_z', 'FT%_z', '3P_z', 'TRB_z', 'AST_z', 'STL_z', 'BLK_z', 'TOV_z', 'PTS_z', 'TEAM'])\n",
    "        #self.pool = createPool(self.poolSize, self.agents)\n",
    "        self.pool, self.playerNames = getNBAPool() #TODO: Switch back to randomized pools\n",
    "        # The state of the pool before the current action\n",
    "        self.prevPool = pd.DataFrame(columns=['ID', 'POS', 'FG%_z', 'FT%_z', '3P_z', 'TRB_z', 'AST_z', 'STL_z', 'BLK_z', 'TOV_z', 'PTS_z', 'TEAM'])\n",
    "        # Used for rendering how rewards is calculated\n",
    "        self.rewardText = \"\"\n",
    "        # Used for rendering the draft picks\n",
    "        self.draftHistory = []\n",
    "\n",
    "    def step(self, action):\n",
    "        # Action is the index of the player being drafted\n",
    "\n",
    "        # Mark the DQN's player as drafted\n",
    "        self.prevPool = self.pool\n",
    "        self.pool.loc[action, 'TEAM'] = 1\n",
    "        self.draftNum += 1\n",
    "        self.draftHistory.append((1, action))\n",
    "\n",
    "        # Dumb agents after the DQN agent drafts\n",
    "        while True: \n",
    "            if self.draftNum >= self.teamCount:\n",
    "                # Draft is done when everyone has 13 players\n",
    "                if len(self.agents[self.draftNum-1].team) >= 13:\n",
    "                    #return np.array([[self.pool.values, self.prevPool.values]]), getReward(self.agents), 1, 0\n",
    "                    reward, self.rewardText = getReward(self.agents)\n",
    "                    return createState(self.pool, self.agents), reward, 1, 0\n",
    "\n",
    "                # Simulate snake draft  \n",
    "                self.agents.reverse()\n",
    "                self.draftNum = 0\n",
    "                # Calculate the reward\n",
    "                reward, self.rewardText = getReward(self.agents)\n",
    "\n",
    "            if self.agents[self.draftNum].type != \"DQN\" and len(self.agents[self.draftNum].team) < 13:\n",
    "                self.prevPool = self.pool\n",
    "                self.pool, ind = self.agents[self.draftNum].draft(self.pool, self.playerNames)\n",
    "                self.draftHistory.append((self.agents[self.draftNum].id, ind))\n",
    "                self.draftNum += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Returns next_state, reward, done, info\n",
    "        return createState(self.pool, self.agents), reward, 0, 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Shuffle draft order\n",
    "        random.shuffle(self.agents)\n",
    "        self.draftNum = 0\n",
    "        self.draftHistory = []\n",
    "        # Randomize the player pool\n",
    "        #self.pool = createPool(self.poolSize, self.agents)\n",
    "        self.pool['TEAM'] = 0\n",
    "        # Clear all teams and return a clean \"state\"\n",
    "        for agent in self.agents:\n",
    "            agent.reset()   \n",
    "        # Draft players for all dumb agents in earlier draft position than DQN agent\n",
    "        while True:\n",
    "            ind = 0\n",
    "            if self.agents[self.draftNum].type != \"DQN\":\n",
    "                self.pool, ind = self.agents[self.draftNum].draft(self.pool, self.playerNames)\n",
    "                self.draftHistory.append((self.agents[self.draftNum].id, ind))\n",
    "                self.draftNum += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return createState(self.pool, self.agents)\n",
    "\n",
    "    def render(self):\n",
    "        print(self.rewardText)\n",
    "        \n",
    "        '''\n",
    "        cnt = 0\n",
    "        for x in range(13):         \n",
    "            print(\"Round {}\".format(x+1))\n",
    "            for y in range(self.teamCount):\n",
    "                print(\"Agent {} drafts {}\".format(self.draftHistory[cnt+y][0], self.playerNames[self.draftHistory[cnt+y][1]]))\n",
    "            print(\"\")\n",
    "            cnt += self.teamCount\n",
    "        '''\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "# Taken from https://keon.io/deep-q-learning/ with some adjustments\n",
    "class MyDQNAgent:\n",
    "    def __init__(self, pool_size, id):\n",
    "        self.type = \"DQN\"\n",
    "        self.id = id\n",
    "        self.team = []   \n",
    "        self.stats = {'FG%_z':0,'3P_z':0,'FT%_z':0,'TRB_z':0,'AST_z':0,'STL_z':0,'BLK_z':0,'TOV_z':0,'PTS_z':0, 'Overall_z':0}   \n",
    "        self.roster = {1:0, 2:0, 3:0, 4:0, 5:0}\n",
    "        # State size = current and prev frame * # of players in the pool * # of variables per player\n",
    "        #self.state_size = 2 * pool_size * 13\n",
    "        # State size = # of players in the pool * # of variables per player\n",
    "        self.state_size = pool_size * 13 + 12 + 12 * 11\n",
    "        self.action_size = pool_size\n",
    "        self.memory = collections.deque(maxlen=8000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 0.01  # exploration rate\n",
    "        self.epsilon_min = 0.01 ###\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Is 256 nodes too much?\n",
    "        model.add(Dense(128, input_shape=(self.state_size,), activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "      \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Agent is only able to choose from undrafted players\n",
    "        # TODO: The agent is also not allowed to draft too many players of a certain position (How to implement this in replay()?)\n",
    "        \n",
    "        res = \"TEAM == 0\"\n",
    "        '''\n",
    "        if self.roster[1] + self.roster[2] >= 6:\n",
    "          res += \" and POS != 1 and POS != 2\"\n",
    "        if self.roster[3] + self.roster[4] >= 6:\n",
    "          res += \" and POS != 3 and POS != 4\"\n",
    "        if self.roster[5] >= 4:\n",
    "          res += \" and POS != 5\"\n",
    "        '''\n",
    "        unp = unpackState(state)\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Get the index of the randomly chosen undrafted player\n",
    "            # Instead of getting a random player, we choose randomly from the top 15 players\n",
    "            # TODO: Try fully random instead of top 15\n",
    "            choice = random.randrange(15)\n",
    "            ind = unp.query(res).nlargest(choice+1,'Overall_z').iloc[[choice]].index.item()\n",
    "\n",
    "        else:\n",
    "            df = unp.copy()\n",
    "            # act_values is the predicted reward if an action is taken\n",
    "            act_values = self.model.predict(np.array([state]))\n",
    "            df[\"act_values\"] = act_values[0]         \n",
    "          \n",
    "            # Get the index of the undrafted player with the largest 'act_value'\n",
    "            ind = df.query(res).nlargest(1, 'act_values').index.item()\n",
    "\n",
    "        # Transform the player dataframe to a dictionary and remove the unnecessary keys\n",
    "        newPlayer = unp.loc[ind].to_dict()\n",
    "        self.team.append(newPlayer.pop('ID')) # Add player id to the agents team\n",
    "        newPlayer.pop('TEAM')\n",
    "        self.roster[newPlayer.pop('POS')] += 1 # Add the players position to the agents roster\n",
    "\n",
    "        # Add the stats of the new player to the current teams stats\n",
    "        for key in self.stats.keys():\n",
    "            self.stats[key] += newPlayer[key]  \n",
    "          \n",
    "        return ind\n",
    "      \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                res = \"TEAM == 0\"\n",
    "                next_df = unpackState(next_state)\n",
    "                act_values = self.model.predict(np.array([next_state]))\n",
    "                next_df[\"act_values\"] = act_values[0]\n",
    "                target = reward + (self.gamma * next_df.query(res)['act_values'].max())              \n",
    "\n",
    "            target_f = self.model.predict(np.array([state]))\n",
    "            #print(\"{} -> {} + {}\".format(target_f[0][action], reward,self.gamma * next_df.query(res)['act_values'].max()))\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def reset(self):\n",
    "        self.type = \"DQN\"\n",
    "        self.id = 1\n",
    "        self.team = []   \n",
    "        self.stats = {'FG%_z':0,'3P_z':0,'FT%_z':0,'TRB_z':0,'AST_z':0,'STL_z':0,'BLK_z':0,'TOV_z':0,'PTS_z':0, 'Overall_z':0}   \n",
    "        self.roster = {1:0, 2:0, 3:0, 4:0, 5:0}\n",
    "    \n",
    "    def save(self):\n",
    "        # Save the model weights\n",
    "        self.model.save_weights('FantasyDraft-DQN.h5')\n",
    "    \n",
    "    def load(self):\n",
    "        self.model.load_weights('FantasyDraft-DQN.h5')\n",
    "        print(\"Succesfully loaded weights!\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded weights!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:38: FutureWarning: `item` has been deprecated and will be removed in a future version\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:69: FutureWarning: `item` has been deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5, score: 706.5699999999999, overall_Z: 24.13097866362247, time: 0:00:00.774695\n",
      "episode: 1/5, score: 36.72000000000003, overall_Z: 17.363789268765515, time: 0:00:01.565669\n",
      "DQN  FG%_z:5.08 3P_z:-2.80 FT%_z:-13.50 TRB_z:11.71 AST_z:2.11 STL_z:10.02 BLK_z:4.30 TOV_z:-4.24 PTS_z:4.68\n",
      "vs 8 FG%_z:-2.53 3P_z:-2.67 FT%_z:-10.36 TRB_z:1.51 AST_z:4.38 STL_z:4.61 BLK_z:0.66 TOV_z:-0.74 PTS_z:-3.65 Won: 4 Lost: 5\n",
      "vs 2 FG%_z:4.02 3P_z:-13.24 FT%_z:-14.90 TRB_z:4.72 AST_z:-5.05 STL_z:5.89 BLK_z:1.53 TOV_z:6.46 PTS_z:-7.80 Won: 5 Lost: 4\n",
      "vs 4 FG%_z:4.01 3P_z:-7.67 FT%_z:-17.83 TRB_z:12.35 AST_z:1.38 STL_z:1.79 BLK_z:-4.38 TOV_z:-3.35 PTS_z:0.85 Won: 5 Lost: 4\n",
      "vs 7 FG%_z:5.24 3P_z:-5.23 FT%_z:-17.60 TRB_z:7.86 AST_z:-9.28 STL_z:1.79 BLK_z:2.63 TOV_z:5.21 PTS_z:-0.83 Won: 5 Lost: 4\n",
      "vs 11 FG%_z:3.43 3P_z:-7.55 FT%_z:-10.37 TRB_z:5.76 AST_z:-7.14 STL_z:5.38 BLK_z:2.41 TOV_z:4.72 PTS_z:-4.63 Won: 5 Lost: 4\n",
      "vs 9 FG%_z:7.49 3P_z:-4.07 FT%_z:-23.12 TRB_z:4.01 AST_z:-2.80 STL_z:8.97 BLK_z:1.31 TOV_z:2.73 PTS_z:-3.43 Won: 5 Lost: 4\n",
      "vs 12 FG%_z:0.70 3P_z:-5.58 FT%_z:-13.49 TRB_z:7.86 AST_z:-1.07 STL_z:6.41 BLK_z:0.88 TOV_z:0.62 PTS_z:-2.10 Won: 5 Lost: 4\n",
      "vs 5 FG%_z:2.03 3P_z:-5.58 FT%_z:-20.48 TRB_z:9.25 AST_z:-0.71 STL_z:6.15 BLK_z:1.97 TOV_z:-4.72 PTS_z:0.95 Won: 5 Lost: 4\n",
      "vs 10 FG%_z:1.46 3P_z:-3.83 FT%_z:-16.43 TRB_z:3.33 AST_z:-4.03 STL_z:4.61 BLK_z:1.31 TOV_z:0.99 PTS_z:-0.51 Won: 5 Lost: 4\n",
      "vs 3 FG%_z:0.93 3P_z:-5.92 FT%_z:-18.21 TRB_z:8.61 AST_z:-2.09 STL_z:8.45 BLK_z:-3.72 TOV_z:0.12 PTS_z:-2.08 Won: 4 Lost: 5\n",
      "vs 6 FG%_z:5.02 3P_z:-4.76 FT%_z:-18.91 TRB_z:6.83 AST_z:-4.89 STL_z:2.31 BLK_z:2.84 TOV_z:3.60 PTS_z:-1.67 Won: 5 Lost: 4\n",
      "episode: 2/5, score: 533.31, overall_Z: 19.3761556986701, time: 0:00:02.315963\n",
      "DQN  FG%_z:8.35 3P_z:-4.66 FT%_z:-14.71 TRB_z:11.79 AST_z:3.74 STL_z:6.69 BLK_z:7.14 TOV_z:-4.73 PTS_z:5.78\n",
      "vs 3 FG%_z:7.11 3P_z:-3.14 FT%_z:-19.03 TRB_z:4.33 AST_z:-6.42 STL_z:1.79 BLK_z:0.00 TOV_z:1.74 PTS_z:-0.23 Won: 4 Lost: 5\n",
      "vs 2 FG%_z:7.15 3P_z:-11.39 FT%_z:-17.20 TRB_z:3.81 AST_z:2.86 STL_z:-3.84 BLK_z:3.28 TOV_z:3.10 PTS_z:-3.47 Won: 5 Lost: 4\n",
      "vs 4 FG%_z:10.13 3P_z:-14.06 FT%_z:-21.43 TRB_z:8.85 AST_z:-4.28 STL_z:5.38 BLK_z:3.28 TOV_z:1.37 PTS_z:-2.68 Won: 5 Lost: 4\n",
      "vs 10 FG%_z:-1.10 3P_z:-3.14 FT%_z:-13.89 TRB_z:4.13 AST_z:-0.05 STL_z:1.79 BLK_z:6.56 TOV_z:3.72 PTS_z:-2.78 Won: 4 Lost: 5\n",
      "vs 6 FG%_z:3.45 3P_z:-4.88 FT%_z:-20.61 TRB_z:3.89 AST_z:3.26 STL_z:0.77 BLK_z:6.78 TOV_z:-1.99 PTS_z:-2.27 Won: 5 Lost: 4\n",
      "vs 11 FG%_z:0.95 3P_z:-6.04 FT%_z:-13.06 TRB_z:8.38 AST_z:2.80 STL_z:0.26 BLK_z:5.69 TOV_z:-2.11 PTS_z:1.06 Won: 6 Lost: 3\n",
      "vs 9 FG%_z:7.49 3P_z:-11.62 FT%_z:-21.54 TRB_z:12.82 AST_z:-5.20 STL_z:-0.26 BLK_z:6.35 TOV_z:1.74 PTS_z:0.21 Won: 5 Lost: 4\n",
      "vs 7 FG%_z:9.85 3P_z:-7.09 FT%_z:-17.00 TRB_z:4.80 AST_z:-1.78 STL_z:2.56 BLK_z:2.63 TOV_z:1.86 PTS_z:-2.01 Won: 5 Lost: 4\n",
      "vs 12 FG%_z:6.77 3P_z:-9.99 FT%_z:-14.36 TRB_z:7.19 AST_z:6.27 STL_z:7.17 BLK_z:-2.41 TOV_z:-5.09 PTS_z:0.41 Won: 5 Lost: 4\n",
      "vs 5 FG%_z:7.94 3P_z:-7.20 FT%_z:-19.69 TRB_z:9.13 AST_z:-5.40 STL_z:-2.31 BLK_z:7.22 TOV_z:1.49 PTS_z:0.39 Won: 5 Lost: 4\n",
      "vs 8 FG%_z:9.73 3P_z:-11.39 FT%_z:-20.24 TRB_z:4.92 AST_z:1.58 STL_z:6.41 BLK_z:2.19 TOV_z:2.23 PTS_z:-1.60 Won: 6 Lost: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:60: FutureWarning: `item` has been deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3/5, score: -689.8599999999997, overall_Z: 13.605445919707662, time: 0:00:03.087795\n",
      "DQN  FG%_z:5.48 3P_z:-3.50 FT%_z:-14.00 TRB_z:9.64 AST_z:4.20 STL_z:6.69 BLK_z:4.95 TOV_z:-5.23 PTS_z:5.37\n",
      "vs 7 FG%_z:-1.02 3P_z:-3.14 FT%_z:-11.33 TRB_z:6.83 AST_z:1.48 STL_z:2.05 BLK_z:-7.00 TOV_z:-3.60 PTS_z:2.03 Won: 4 Lost: 5\n",
      "vs 6 FG%_z:1.16 3P_z:-9.06 FT%_z:-15.54 TRB_z:5.48 AST_z:-2.09 STL_z:1.02 BLK_z:-0.88 TOV_z:3.10 PTS_z:-1.39 Won: 4 Lost: 5\n",
      "vs 2 FG%_z:-2.70 3P_z:-2.90 FT%_z:-12.99 TRB_z:0.64 AST_z:1.48 STL_z:0.00 BLK_z:-4.59 TOV_z:-1.74 PTS_z:0.56 Won: 4 Lost: 5\n",
      "vs 12 FG%_z:7.02 3P_z:-10.69 FT%_z:-23.49 TRB_z:7.74 AST_z:2.55 STL_z:4.87 BLK_z:6.13 TOV_z:2.36 PTS_z:-8.26 Won: 6 Lost: 3\n",
      "vs 11 FG%_z:3.41 3P_z:-7.67 FT%_z:-15.78 TRB_z:6.67 AST_z:-0.36 STL_z:3.33 BLK_z:0.22 TOV_z:0.62 PTS_z:-1.69 Won: 5 Lost: 4\n",
      "vs 10 FG%_z:0.86 3P_z:-1.16 FT%_z:-12.39 TRB_z:0.75 AST_z:-2.65 STL_z:-0.51 BLK_z:2.41 TOV_z:1.49 PTS_z:-0.48 Won: 4 Lost: 5\n",
      "vs 4 FG%_z:2.66 3P_z:-7.44 FT%_z:-22.01 TRB_z:8.06 AST_z:0.41 STL_z:0.51 BLK_z:8.53 TOV_z:-4.84 PTS_z:-1.43 Won: 5 Lost: 4\n",
      "vs 8 FG%_z:8.20 3P_z:-5.46 FT%_z:-17.25 TRB_z:3.65 AST_z:-5.91 STL_z:-0.51 BLK_z:1.75 TOV_z:4.47 PTS_z:-0.41 Won: 4 Lost: 5\n",
      "vs 3 FG%_z:5.63 3P_z:-9.29 FT%_z:-14.74 TRB_z:0.87 AST_z:2.14 STL_z:-0.26 BLK_z:-1.97 TOV_z:2.36 PTS_z:-4.02 Won: 4 Lost: 5\n",
      "vs 5 FG%_z:6.54 3P_z:-8.48 FT%_z:-21.87 TRB_z:4.29 AST_z:-2.24 STL_z:1.54 BLK_z:5.47 TOV_z:0.87 PTS_z:-1.14 Won: 5 Lost: 4\n",
      "vs 9 FG%_z:3.73 3P_z:-7.67 FT%_z:-20.56 TRB_z:0.87 AST_z:3.52 STL_z:5.38 BLK_z:3.94 TOV_z:-1.12 PTS_z:-3.52 Won: 5 Lost: 4\n",
      "episode: 4/5, score: 659.89, overall_Z: 19.102951041784646, time: 0:00:03.915993\n",
      "DQN  FG%_z:11.09 3P_z:-7.80 FT%_z:-20.43 TRB_z:13.30 AST_z:9.40 STL_z:8.99 BLK_z:8.46 TOV_z:-9.45 PTS_z:5.55\n",
      "vs 6 FG%_z:9.48 3P_z:-13.13 FT%_z:-18.94 TRB_z:6.15 AST_z:6.68 STL_z:2.56 BLK_z:2.84 TOV_z:-4.22 PTS_z:-1.58 Won: 5 Lost: 4\n",
      "vs 12 FG%_z:5.28 3P_z:-10.46 FT%_z:-24.50 TRB_z:5.16 AST_z:5.81 STL_z:11.53 BLK_z:7.22 TOV_z:-2.98 PTS_z:-3.59 Won: 5 Lost: 4\n",
      "vs 3 FG%_z:10.20 3P_z:-12.66 FT%_z:-23.79 TRB_z:9.65 AST_z:5.56 STL_z:0.51 BLK_z:0.66 TOV_z:-4.10 PTS_z:-0.14 Won: 5 Lost: 4\n",
      "vs 2 FG%_z:6.48 3P_z:-7.20 FT%_z:-20.05 TRB_z:6.19 AST_z:2.80 STL_z:2.82 BLK_z:-2.41 TOV_z:-1.99 PTS_z:-3.06 Won: 4 Lost: 5\n",
      "vs 7 FG%_z:12.33 3P_z:-16.03 FT%_z:-25.70 TRB_z:11.35 AST_z:4.03 STL_z:2.05 BLK_z:8.53 TOV_z:-2.23 PTS_z:-4.24 Won: 5 Lost: 4\n",
      "vs 4 FG%_z:6.51 3P_z:-10.46 FT%_z:-27.51 TRB_z:11.95 AST_z:7.55 STL_z:8.45 BLK_z:3.28 TOV_z:-10.18 PTS_z:1.64 Won: 6 Lost: 3\n",
      "vs 10 FG%_z:9.37 3P_z:-12.43 FT%_z:-21.90 TRB_z:7.42 AST_z:3.82 STL_z:3.07 BLK_z:7.22 TOV_z:-1.86 PTS_z:-2.29 Won: 5 Lost: 4\n",
      "vs 11 FG%_z:11.23 3P_z:-10.46 FT%_z:-24.12 TRB_z:6.19 AST_z:6.12 STL_z:6.41 BLK_z:7.22 TOV_z:-5.71 PTS_z:1.37 Won: 6 Lost: 3\n",
      "vs 5 FG%_z:10.92 3P_z:-11.73 FT%_z:-25.95 TRB_z:10.44 AST_z:6.63 STL_z:-1.79 BLK_z:6.56 TOV_z:-5.09 PTS_z:-1.69 Won: 4 Lost: 5\n",
      "vs 9 FG%_z:9.26 3P_z:-9.76 FT%_z:-26.24 TRB_z:7.34 AST_z:3.37 STL_z:7.43 BLK_z:9.85 TOV_z:-3.48 PTS_z:-0.83 Won: 5 Lost: 4\n",
      "vs 8 FG%_z:12.29 3P_z:-8.95 FT%_z:-23.48 TRB_z:7.70 AST_z:3.47 STL_z:-1.02 BLK_z:5.69 TOV_z:-2.98 PTS_z:-2.27 Won: 4 Lost: 5\n"
     ]
    }
   ],
   "source": [
    "# TODO: Multiagent Training\n",
    "# TODO: Reward function should be much bigger for the final round\n",
    "\n",
    "# Looks like the agent is learning to punt 3P, FT%, and TO\n",
    "# TODO: Try running the agent with the 2017-2018 stats.\n",
    "\n",
    "# Training the DQN agent\n",
    "\n",
    "# Initialize the environment\n",
    "poolSize = 280\n",
    "numRounds = 13\n",
    "numTeams = 12\n",
    "agent = MyDQNAgent(poolSize, 1)\n",
    "agent.load()\n",
    "env = DraftEnvironment(poolSize, agent, numTeams)\n",
    "numEpisodes = 5\n",
    "done = 0\n",
    "\n",
    "metricsEvery = 1\n",
    "renderEvery = 1\n",
    "roundScore = 0\n",
    "roundOverall = 0\n",
    "avgScore = []\n",
    "avgOverall = []\n",
    "\n",
    "fo = open(\"Logs-FantasyDraft-DQN.txt\", \"w\")\n",
    "\n",
    "timeStart = time.time()\n",
    "# Iterate the draft\n",
    "for e in range(numEpisodes):\n",
    "  \n",
    "    # Reset environment\n",
    "    state = env.reset()\n",
    "  \n",
    "    # Until the drafting process is done\n",
    "    for r in range(numRounds):\n",
    "    \n",
    "        # DQN Agent decides action\n",
    "        action = agent.act(state)\n",
    "        # Move to the next state given the DQN Agents actiono\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # Save the experience to memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        # Update current state\n",
    "        state = next_state\n",
    "        \n",
    "        roundScore += reward\n",
    "    \n",
    "    # Print the agents score\n",
    "    avgScore.append(roundScore)\n",
    "    roundScore = 0\n",
    "    avgOverall.append(agent.stats['Overall_z'])\n",
    "        \n",
    "    if e % metricsEvery == 0:\n",
    "        text = \"episode: {}/{}, score: {}, overall_Z: {}, time: {}\".format(e, numEpisodes, sum(avgScore)/len(avgScore), sum(avgOverall)/len(avgOverall), datetime.timedelta(seconds=time.time() - timeStart))\n",
    "        print(text)\n",
    "        fo.write(text+'\\n')\n",
    "        avgScore = []\n",
    "        avgOverall = []\n",
    "        agent.save()\n",
    "\n",
    "    if e % renderEvery == 0 and e > 0:\n",
    "        env.render()\n",
    "            \n",
    "    if len(agent.memory) > 128*10:\n",
    "        # Train the agent using its experiences\n",
    "        agent.replay(128)\n",
    "        \n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in env.agents:\n",
    "    print(agent.id)\n",
    "    print(len(agent.team))\n",
    "    print(agent.stats)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.pool.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poolSize = 280\n",
    "numRounds = 13\n",
    "numTeams = 12\n",
    "agent = MyDQNAgent(poolSize, 1)\n",
    "agent.load()\n",
    "env = DraftEnvironment(poolSize, agent, numTeams)\n",
    "# Reset environment\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent decides action\n",
    "action = agent.act(state)\n",
    "# Move to the next state given the DQN Agents action\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "agent.remember(state, action, reward, next_state, done)\n",
    "# Update current state\n",
    "state = next_state\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['act_values'].max()\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "print(env.agents[x].team)\n",
    "print(env.agents[x].stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+=1\n",
    "print(env.agents[x].team)\n",
    "print(env.agents[x].stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script FantasyDraft-DQN.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
